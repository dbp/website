<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>dbp.io :: essays</title>
        <link>http://dbp.io</link>
        <description><![CDATA[writing on programming etc by daniel patterson]]></description>
        <atom:link href="http://dbp.io/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Thu, 19 Apr 2018 00:00:00 UT</lastBuildDate>
        <item>
    <title>How to prove a compiler fully abstract</title>
    <link>http://dbp.io/essays/2018-04-19-how-to-prove-a-compiler-fully-abstract.html</link>
    <description><![CDATA[<h2>How to prove a compiler fully abstract</h2>

<p>by <em>Daniel Patterson</em> on <strong>April 19, 2018</strong></p>

<p>A compiler that preserves and reflects equivalences is called a <strong>fully abstract</strong> compiler. This is a powerful property for a compiler that is different (but complimentary) to the more common notion of <a href="/essays/2018-01-16-how-to-prove-a-compiler-correct.html">compiler correctness</a>. So what does it mean, and how do we prove it?</p>
<blockquote>
<p>All the code for this post, along with instructions to get it running, is in the repository <a href="https://github.com/dbp/howtoprovefullabstraction" class="uri">https://github.com/dbp/howtoprovefullabstraction</a>. If you have any trouble getting it going, please open an issue on that repository and I’ll help figure it out with you.</p>
</blockquote>
<p>Both <strong>equivalence preservation</strong> and <strong>equivalence reflection</strong> (what make a compiler fully abstract) relate to how the compiler treats program equivalences, which in this case I’m considering observational equivalence. Two programs <code>p1</code> and <code>p2</code> are <strong>observationally equivalent</strong> if you cannot tell any difference between the result of running them, including any side effects.</p>
<p>For example, if the only observable behavior about programs in your language that you can make is see what output they print, this means that the two programs print the same output, even if they are implemented in completely different ways. Observational equivalence is extremely useful, especially for compilers, which when optimizing may change how a particular program is implemented but should not change the observable behavior. But it is also useful for programmers, who commonly refactor code, which means they change how the code is implemented (to make it easier to maintain, or extend, or better support some future addition), without changing any functionality. <em>Refactoring is an equivalence-preserving transformation.</em> We write observational equivalence on programs formally as:</p>
<pre><code>p1 ≈ p1</code></pre>
<h3 id="contextual-equivalence">Contextual equivalence</h3>
<p>But we often also want to compile not just whole programs, but particular modules, expressions, or in the general sense, <strong>components</strong>, and in that case, we want an analogous notion of equivalence. Two components are <strong>contextually equivalent</strong> if in all program contexts they produce the same observable behavior. In other words, if you have two modules, but any way you combine those modules with the rest of a program (so the rest is syntactically identical, but the modules differ), the results are observationally equivalent, then those two modules are contextually equivalent. We will write this, overloading the <code>≈</code> for both observational and contextual equivalence, as:</p>
<pre><code>e1 ≈ e1</code></pre>
<p>As an example, if we consider a simple functional language and consider our components to be individual expressions, it should be clear that these two expressions are contextually equivalent:</p>
<pre><code>λx. x * 2 ≈ λx. x + x</code></pre>
<p>While they are implemented differently, no matter how they are used, the result will always be the same (as the only thing we can do with these functions is call them on an argument, and when we do, each will double its argument, even though in a different way). It’s important to note that contextual equivalence always depends on what is observable within the language. For example, in Javascript, you can reflect over the syntax of functions, and so the above two functions, written as:</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">function</span>(x)<span class="op">{</span> <span class="cf">return</span> x <span class="op">*</span> <span class="dv">2</span><span class="op">;</span> <span class="op">}</span> ≈ <span class="kw">function</span>(x)<span class="op">{</span> <span class="cf">return</span> x <span class="op">+</span> x<span class="op">;</span> <span class="op">}</span></code></pre></div>
<p>Would not be contextually equivalent, because there exists a program context that can distinguish them. What is that context? Well, if we imagine plugging in the functions above into the “hole” written as <code>[·]</code> below, the result will be different for the two functions! This is because the <code>toString()</code> method on functions in Javascript returns the source code of the function.</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript">([·]).<span class="at">toString</span>()</code></pre></div>
<p>From the perspective of optimizations, this is troublesome, as you can’t be sure that a transformation between the above programs was safe (assuming one was much faster than the other), as there could be code that relied upon the particular way that the source code had been written. There are more complicated things you can do (like optimizing speculatively and falling back to unoptimized versions when reflection was needed). In general though, languages with that kind of reflection are both harder to write fast compilers for and harder to write secure compilers for, and while it’s not the topic of this post, it’s always important to know what you mean by contextual equivalence, which usually means: <em>what can program contexts determine about components</em>.</p>
<h3 id="part-1.-equivalence-reflection">Part 1. Equivalence reflection</h3>
<p>With that in mind, what does <strong>equivalence reflection</strong> and <strong>equivalence preservation</strong> for a compiler mean? Let’s start with <strong>equivalence reflection</strong>, as that’s the property that all your correct compilers already have. Equivalence reflection means that if two components, when compiled, are equivalent, then the source components must have been equivalent. We can write this more formally as (where we write <code>s ↠ t</code> to mean a component <code>s</code> is compiled to <code>t</code>):</p>
<pre><code>∀ s1 s2 t1 t2. s1 ↠ t1 ∧ s2 ↠ t2 ∧ t1 ≈ t2 ⇒ s1 ≈ s2</code></pre>
<p>What are the consequences of this definition? And why do correct compilers have this property? Well, the contrapositive is actually easier to understand: it says that if the source components weren’t equivalent then the target components would have to be different, or more formally:</p>
<pre><code>∀ s1 s2 t1 t2. s1 ↠ t1 ∧ s2 ↠ t2 ∧ s1 ≉ s2 ⇒ t1 ≉ t2</code></pre>
<p>If this didn’t hold, then the compiler could take different source components and compile them to the same target component! Which means you could have different source programs you wrote, which have observationally different behavior, and your compiler would produce the same target program! Any correct compiler has to preserve observational behavior, and it couldn’t do that in this case, as the target program only has one behavior, so it can’t have both the behavior of <code>s1</code> and <code>s2</code> (for pedants, not considering non-deterministic targets).</p>
<p>So equivalence reflection should be thought of as related to compiler correctness. Note, however, that equivalence reflection is <em>not</em> the same as compiler correctness: as long as your compiler produced different target programs for different source programs, all would be fine – your compiler could hash the source program and produce target programs that just printed the hash to the screen, and it would be equivalence reflecting, since it would produce different target programs not only for source programs that were observationally different, but even syntactically different! That would be a pretty bad compiler, and certainly not correct, but it would be equivalence reflecting.</p>
<h3 id="part-2.-equivalence-preservation">Part 2. Equivalence preservation</h3>
<p>Equivalence preservation, on the other hand, is the hallmark of fully abstract compilers, and it is a property that even most correct compilers do not have, though it would certainly be great if they did. It says that if two source components are equivalent, then the compiled versions must still be equivalent. Or, more formally:</p>
<pre><code>∀ s1 s2 t1 t2. s1 ↠ t1 ∧ s2 ↠ t2 ∧ s1 ≈ s2 ⇒ t1 ≈ t2</code></pre>
<p>(See, I just reversed the implication. Neat trick! But now it means something totally different). One place where this has been studied extensively is by security researchers, because what it tells you is that observers in the target can’t make observations that aren’t possible to distinguish in the source. Let’s make that a lot more concrete, where we will also see why it’s not frequently true, even of proven correct compilers.</p>
<p>Say your language has some information hiding feature, like a private field, and you have two source components that are identical except they have different values stored in the private field. If the compiler does not preserve the fact that it is private (because, for example, it translates the higher level object structure into a C struct or just a pile of memory accessed by assembly), then other target code could read the private values, and these two components will no longer be equivalent.</p>
<p>This also has implications for programmer refactoring and compiler optimizations: I (or my compiler) might think that it is safe to replace one version of the program with another, because I know that in my language these are equivalent, but what I don’t know is that the compiler reveals distinguishing characteristics, and perhaps some target-level library that I’m linking with relies upon details (that were supposed to be hidden) of how the old code worked. If that’s the case, I can have a working program, and make a change that does not change the meaning of the component <em>in my language</em>, but the whole program can no longer work.</p>
<p>Proving a compiler fully abstract, therefore, is all about proving equivalence preservation. So how do we do it?</p>
<h3 id="how-to-prove-equivalence-preservation">How to prove equivalence preservation</h3>
<p>Looking at what we have to prove, we see that given contextually equivalent source components <code>s1</code> and <code>s2</code>, we need to show that <code>t1</code> and <code>t2</code> are contextually equivalent. We can expand this to explicitly quantify over the contexts that combine with the components to make whole programs:</p>
<pre><code>∀ s1 s2 t1 t2. s1 ↠ t1 ∧ s2 ↠ t2 ∧ (∀Cs. Cs[s1] ≈ Cs[s2]) ⇒ (∀Ct. Ct[t1] ≈ Ct[t2])</code></pre>
<p>Noting that as mentioned above, I am overloading <code>≈</code> to now mean whole-program observational equivalence (so, running the program produces the same observations).</p>
<p>First I’ll outline how the proof will go in general, and then we’ll consider an actual example compiler and do the proof for the concrete example.</p>
<p>We can see that in order to prove this, we need to consider an arbitrary target context <code>Ct</code> and show that <code>Ct[t1]</code> and <code>Ct[t2]</code> are observationally equivalent. We do this by showing that <code>Ct[t1]</code> is observationally equivalent to <code>Cs'[s1]</code> – that is, we produce a source context <code>Cs'</code> that we claim is equivalent to <code>Ct</code>. We do this by way of a “back-translation”, which will be a sort of compiler in reverse. Assuming that we can produce such a <code>Cs'</code> and that <code>Cs'[s1]</code> and <code>Ct[t1]</code> (and correspondingly <code>Cs'[s2]</code> and <code>Ct[t2]</code>) are indeed observationally equivalent (noting that this relies upon a cross-language notion of observations), we can prove that <code>Ct[t1]</code> and <code>Ct[t2]</code> are observationally equivalent by instantiating our hypothesis <code>∀Cs. Cs[s1] ≈ Cs[s2]</code> with <code>Cs'</code>. This tells us that <code>Cs'[s1] ≈ Cs'[s2]</code>, and by transitivity, <code>Ct[t1] ≈ Ct[t2]</code>.</p>
<p>It can be helpful to see it is a diagram, where the top line is given by the hypothesis (once instantiated with the source context we come up with by way of backtranslation) and coming up with the back-translation and showing that <code>Ct</code> and <code>Cs'</code> are equivalent is the hard part of the proof.</p>
<pre><code>Cs&#39;[s1]  ≈  Cs&#39;[s2]
  ≈           ≈
Ct[t1]   ?  Ct[t2]</code></pre>
<h3 id="concrete-example-of-languages-compiler-proof-of-full-abstraction">Concrete example of languages, compiler, &amp; proof of full abstraction</h3>
<p>Let’s make this concrete with an example. This will be presented some in english and some in the proof assistant Coq. This post isn’t an introduction to Coq; for that, see e.g., Bertot and Casteron’s Coq’Art, Chlipala’s CPDT, or Pierce et al’s Software Foundations.</p>
<p>Our source language is arithmetic expressions over integers with addition and subtraction:</p>
<pre><code>e ::= n
    | e + e
    | e - e</code></pre>
<p>This is written down in Coq as:</p>
<pre class="coq"><code>Inductive Expr : Set := 
  | Num : Z -&gt; Expr
  | Plus : Expr -&gt; Expr -&gt; Expr 
  | Minus : Expr -&gt; Expr -&gt; Expr.</code></pre>
<p>Evaluation is standard (if you wanted to parse this, you would need to deal with left/right associativity, and probably add parenthesis to disambiguate, but we consider the point where you already have a tree structure, so it is unambiguous). We can write the evaluation function as:</p>
<pre class="coq"><code>Fixpoint eval_Expr (e : Expr) : Z := 
  match e with
  | Num n =&gt; n                               
  | Plus e1 e2 =&gt; eval_Expr e1 + eval_Expr e2
  | Minus e1 e2 =&gt; eval_Expr e1 - eval_Expr e2
  end.</code></pre>
<p>Our target language is a stack machine which uses a stack of integers to evaluate the sequence of instructions. In addition to having instructions to add and subtract, our stack machine has an extra instruction: <code>OpCount</code>. This instruction returns how many operations remain on the stack machine, and it puts that integer on the top of the stack. This is the simplest abstraction I could think of that will provide an interesting case study for problems of full abstraction, and is a stand-in for both reflection (as it allows the program to inspect other parts of the program), and also somewhat of a proxy for execution time (remaining). Our stack machine requires that the stack be empty at the end of execution.</p>
<pre class="coq"><code>Inductive Op : Set :=
| Push : Z -&gt; Op
| Add : Op
| Sub : Op
| OpCount : Op.</code></pre>
<p>Let’s see the compiler and the evaluation function (note that we reverse the order when we pop values off the stack from when we put them on in the compiler).</p>
<pre class="coq"><code>Fixpoint compile_Expr (e : Expr) : list Op :=
  match e with
  | Num n =&gt; [Push n]
  | Plus e1 e2 =&gt; compile_Expr e1 ++ compile_Expr e2 ++ [Add]
  | Minus e1 e2 =&gt; compile_Expr e1 ++ compile_Expr e2 ++ [Sub]
  end.

Fixpoint eval_Op (s : list Z) (ops : list Op) : option Z :=
  match (ops, s) with
  | ([], [n]) =&gt; Some n
  | (Push z :: rest, _) =&gt; eval_Op (z :: s) rest 
  | (Add :: rest, n2 :: n1 :: ns) =&gt; eval_Op (n1 + n2 :: ns)%Z rest
  | (Sub :: rest, n2 :: n1 :: ns) =&gt; eval_Op (n1 - n2 :: ns)%Z rest
  | (OpCount :: rest, _) =&gt; eval_Op (Z.of_nat (length rest) :: s) rest
  | _ =&gt; None
  end.</code></pre>
<p>We can prove a basic (<em>whole program</em>) compiler correctness result for this (for more detail on this type of result, see <a href="/essays/2018-01-16-how-to-prove-a-compiler-correct.html">this post</a>), where first we prove a general <code>eval_step</code> lemma and then use that to prove correctness (note: the <code>hint</code> and <code>hint_rewrite</code> tactics are from an experimental <a href="https://github.com/dbp/literatecoq">literatecoq</a> library that adds support for proof-local hinting, which some might think is a hack but I think makes the proofs much more readable/maintainable).</p>
<pre class="coq"><code>Lemma eval_step : forall a : Expr, forall s : list Z, forall xs : list Op,
      eval_Op s (compile_Expr a ++ xs) = eval_Op (eval_Expr a :: s) xs.
Proof.
  hint_rewrite List.app_assoc_reverse.
  induction a; intros; iauto; simpl;
  hint_rewrite IHa2, IHa1;
  iauto&#39;.
Qed.

Theorem compiler_correctness : forall a : Expr,
    eval_Op [] (compile_Expr a) = Some (eval_Expr a).
Proof.
  hint_rewrite eval_step.
  hint_simpl.
  induction a; iauto&#39;.
Qed.</code></pre>
<p>Now, before we can state properties about equivalences, we need to define what we mean by equivalence for our source and target languages. Both produce no side effects, so the only observation is the end result. Thus, observational equivalence is pretty straightforward; it follows from evaluation:</p>
<pre class="coq"><code>Definition equiv_Expr (e1 e2 : Expr) : Prop := eval_Expr e1 = eval_Expr e2.
Definition equiv_Op (p1 p2 : list Op) : Prop := eval_Op [] p1 = eval_Op [] p2.</code></pre>
<p>But, we want to talk not just about whole programs, but about partial programs that can get linked with other parts to create whole programs. In order to do that, we create a new type of “evaluation context” for our <code>Expr</code>, that has a hole (typically written on paper as <code>[·]</code>). This is a program that is missing an expression, which must be filled into the hole. Given how simple our language is, any expression can be filled in to the hole and that will produce a valid program. We only want to have <em>one</em> hole per partial program, so in the cases for <code>+</code> and <code>-</code>, one branch must be a normal <code>Expr</code> (so it contains no hole), and the other can contain one hole. Our <code>link_Expr</code> function takes a context and an expression and fills in the hole.</p>
<pre class="coq"><code>Inductive ExprCtxt : Set := 
| Hole : ExprCtxt
| Plus1 : ExprCtxt -&gt; Expr -&gt; ExprCtxt
| Plus2 : Expr -&gt; ExprCtxt -&gt; ExprCtxt 
| Minus1 : ExprCtxt -&gt; Expr -&gt; ExprCtxt
| Minus2 : Expr -&gt; ExprCtxt -&gt; ExprCtxt.

Fixpoint link_Expr (c : ExprCtxt) (e : Expr) : Expr :=
  match c with
  | Hole =&gt; e
  | Plus1 c&#39; e&#39; =&gt; Plus (link_Expr c&#39; e) e&#39;
  | Plus2 e&#39; c&#39; =&gt; Plus e&#39; (link_Expr c&#39; e)
  | Minus1 c&#39; e&#39; =&gt; Minus (link_Expr c&#39; e) e&#39;
  | Minus2 e&#39; c&#39; =&gt; Minus e&#39; (link_Expr c&#39; e)
  end.</code></pre>
<p>For our stack machine, partial programs are much easier, since a program is just a list of <code>Op</code>, which means that any program can be extended by adding new <code>Op</code>s on either end (or inserting in the middle).</p>
<p>With <code>ExprCtxt</code>, we can now define “contextual equivalence” for our source language:</p>
<pre class="coq"><code>Definition ctxtequiv_Expr (e1 e2 : Expr) : Prop :=
  forall c : ExprCtxt, eval_Expr (link_Expr c e1) = eval_Expr (link_Expr c e2).</code></pre>
<p>We can do the same with our target, simplifying slightly and saying that we will allow adding arbitrary <code>Op</code>s before and after, but not in the middle, of an existing sequence of <code>Op</code>s.</p>
<pre class="coq"><code>Definition ctxtequiv_Op (p1 p2 : list Op) : Prop :=
  forall c1 c2 : list Op, eval_Op [] (c1 ++ p1 ++ c2) = eval_Op [] (c1 ++ p2 ++ c2).</code></pre>
<p>To prove our compiler fully abstract, remember we need to prove that it preserves and reflects equivalences. Since we already proved that it is correct, proving that it reflects equivalences should be relatively straightforward, so lets start there. The lemma we want is:</p>
<pre class="coq"><code>Lemma equivalence_reflection :
  forall e1 e2 : Expr,
  forall p1 p2 : list Op,
  forall comp1 : compile_Expr e1 = p1,
  forall comp2 : compile_Expr e2 = p2,
  forall eqtarget : ctxtequiv_Op p1 p2,
    ctxtequiv_Expr e1 e2.
Proof.
  unfold ctxtequiv_Expr, ctxtequiv_Op in *.
  intros.
  induction c; simpl; try solve [hint_rewrite IHc; iauto];
    (* NOTE(dbp 2018-04-16): Only the base case, for Hole, remains *)
    [idtac].
  (* NOTE(dbp 2018-04-16): In the hole case, specialize the target ctxt equiv hypothesis to empty *)
  specialize (eqtarget [] []); simpl in eqtarget; repeat rewrite app_nil_r in eqtarget.

  (* NOTE(dbp 2018-04-16): At this point, we know e1 -&gt; p1, e2 -&gt; p2, &amp; p1 ≈ p2,
  and want e1 ≈ e2, which follows from compiler correctness *)
  rewrite &lt;- comp1 in eqtarget. rewrite &lt;- comp2 in eqtarget.
  repeat rewrite compiler_correctness in eqtarget.
  inversion eqtarget.
  reflexivity.
Qed.</code></pre>
<p>This lemma is a little more involved, but not by much; we proceed by induction on the structure of the evaluation contexts, and in all but the case for <code>Hole</code>, the induction hypothesis gives us exactly what we need. In the base case, we need to appeal to the <code>compiler_correctness</code> lemma we proved earlier, but otherwise it follows easily.</p>
<p>So what about equivalence preservation? We can state the lemma quite easily:</p>
<pre class="coq"><code>Lemma equivalence_preservation :
  forall e1 e2 : Expr,
  forall p1 p2 : list Op,
  forall comp1 : compile_Expr e1 = p1,
  forall comp2 : compile_Expr e2 = p2,
  forall eqsource : ctxtequiv_Expr e1 e2,
    ctxtequiv_Op p1 p2.
Proof.
Abort.</code></pre>
<p>But proving it is another matter. In fact, it’s not provable, because it’s not true. We can come up with a counter-example, using that <code>OpCount</code> instruction we (surreptitiously) added to our target language. These two expressions are contextually equivalent in our source language (should be obvious, but putting a proof):</p>
<pre class="coq"><code>Example src_equiv : ctxtequiv_Expr (Plus (Num 1) (Num 1)) (Num 2).
Proof.
  unfold ctxtequiv_Expr.
  induction c; simpl; try rewrite IHc; iauto.
Qed.</code></pre>
<p>But they are not contextually equivalent in the target; in particular, if we put the <code>OpCount</code> instruction before and then the <code>Add</code> instruction afterwards, the result will be the value plus the number of instructions it took to compute it:</p>
<pre class="coq"><code>Example target_not_equiv :
  eval_Op [] (OpCount :: compile_Expr (Plus (Num 1) (Num 1)) ++ [Add]) &lt;&gt;
  eval_Op [] (OpCount :: compile_Expr (Num 2) ++ [Add]).
Proof.
  simpl.
  congruence.
Qed.</code></pre>
<p>The former evaluating to <code>6</code>, while the latter evaluates to <code>4</code>. This means that there is no way we are going to be able to prove equivalence preservation (as we have a counter-example!).</p>
<p>So what do we do? Well, this scenario is not uncommon, and it’s the reason why many, even correct, compilers are not fully abstract. It’s also related to why many of these compilers may still have security problems! The solution is to somehow protect the compiled code from having the equivalences disrupted. If this were a real machine, we might want to have some flag on instructions that meant that they should not be counted, and <code>OpCount</code> would just not return anything if it saw any of those (or would count them as 0). Alternately, we might give our target language a type system that is able to rule out linking with code that uses the <code>OpCount</code> instruction, or perhaps restricts how it can be used.</p>
<p>Because this is a blog-post sized example, and I wanted to keep the proofs as short as possible, and the unstructured and untyped nature of our target (which, indeed, is much less structured than our source language; the fact that the source is so well-structured is why our whole-program correctness result was so easy!) will mean the proofs get relatively complex (or require us to add various auxiliary definitions), so the solution I’m going to take is somewhat extreme. Rather than, say, restricting how <code>OpCount</code> is used, or even ruling out linking with <code>OpCount</code>, we’re going to highly restrict what we can link with. This is very artificial, and done entirely so that the proofs can fit into a few lines. In this case, rather than a list, we are going to allow one <code>Op</code> before and one <code>Op</code> after our compiled program, neither of which can be <code>OpCount</code>, and further, we still want the resulting program to be well-formed (i.e., no errors, only one number on stack at end), so either there should be nothing before and after, or there is a <code>Push n</code> before and either <code>Add</code> or <code>Sub</code> after. (You should be able to verify that no other combination of <code>Op</code> before or after will fulfill our requirement).</p>
<p>We can define these possible linking contexts and a helper to combine them with programs as the following:</p>
<pre class="coq"><code>Inductive OpCtxt : Set :=
| PushAdd : Z -&gt; OpCtxt
| PushSub : Z -&gt; OpCtxt
| Empty : OpCtxt.

Definition link_Op  (c : OpCtxt) (p : list Op) : list Op :=
  match c with
  | PushAdd n =&gt; Push n :: p ++ [Add]
  | PushSub n =&gt; Push n :: p ++ [Sub]
  | Empty =&gt; p
  end.</code></pre>
<p>Using that, we can redefine contextual equivalence for our target language, only permitting these contexts:</p>
<pre class="coq"><code>Definition ctxtequiv_Op (p1 p2 : list Op) : Prop :=
  forall c : OpCtxt, eval_Op [] (link_Op c p1) = eval_Op [] (link_Op c p2).
</code></pre>
<p>The only change to our proof of equivalence reflection is on one line, to change our specialization of the target contexts, now to the <code>Empty</code> context:</p>
<pre class="coq"><code>specialize (eqtarget Empty) (* Empty rather than [] [] *)</code></pre>
<p>With that change, we now believe that our compiler, when linked against these restricted contexts, is indeed fully abstract. So let’s prove it. If you recall from earlier in this post, proving equivalence preservation means proving that the bottom line implies the top, in the following diagram:</p>
<pre><code>Cs&#39;[s1]  ≈  Cs&#39;[s2]
  ≈           ≈
Ct[t1]   ?  Ct[t2]</code></pre>
<p>In order to do that, we rely upon a backtranslation to get from <code>Ct</code> to <code>Cs'</code>, where <code>Ct</code> is a target context, in this tiny example our restricted <code>OpCtxt</code>. We can write that backtranslation as:</p>
<pre class="coq"><code>Definition backtranslate (c : OpCtxt) : ExprCtxt :=
  match c with
  | PushAdd n =&gt; Plus2 (Num n) Hole
  | PushSub n =&gt; Minus2 (Num n) Hole
  | Empty =&gt; Hole
  end.</code></pre>
<p>The second part of the proof is showing that the vertical equivalences in the diagram hold — that is, that if <code>s1</code> is compiled to <code>t1</code> and <code>Ct</code> is backtranslated to <code>Cs'</code> then <code>Ct[t1]</code> is equivalent to <code>Cs'[s1]</code>. We can state and prove that as the following lemma, which follows from straightforward case analysis on the structure of our target context and backtranslation (using our <code>eval_step</code> lemmas):</p>
<pre class="coq"><code>Lemma back_translation_equiv :
  forall c : OpCtxt,
  forall p : list Op,
  forall e : Expr,
  forall c&#39; : ExprCtxt, 
    compile_Expr e = p -&gt;
    backtranslate c = c&#39; -&gt;
    eval_Op [] (link_Op c p) = Some (eval_Expr (link_Expr c&#39; e)).
Proof.
  hint_rewrite eval_step, eval_step&#39;.
  intros.
  match goal with
  | [ c : OpCtxt |- _] =&gt; destruct c
  end; 
    match goal with
    | [ H : backtranslate _ = _ |- _] =&gt; invert H
    end; simpl; iauto. 
Qed.</code></pre>
<p>Once we have that lemma, we can prove equivalence preservation directly. We do this by doing case analysis on the target context we are given, backtranslating it and then using the lemma we just proved to get the equivalence that we need.</p>
<pre class="coq"><code>Lemma equivalence_preservation :
  forall e1 e2 : Expr,
  forall p1 p2 : list Op,
  forall comp1 : compile_Expr e1 = p1,
  forall comp2 : compile_Expr e2 = p2,
  forall eqsource : ctxtequiv_Expr e1 e2,
    ctxtequiv_Op p1 p2.
Proof.
  unfold ctxtequiv_Expr, ctxtequiv_Op in *.
  intros.

  remember (backtranslate c) as c&#39;.
  destruct c; iauto;

  erewrite back_translation_equiv with (e := e1) (c&#39; := c&#39;); iauto;
  erewrite back_translation_equiv with (e := e2) (c&#39; := c&#39;); iauto;
  specialize (eqsource c&#39;); simpl in *; congruence.
Qed.</code></pre>
<p>This was obviously a very tiny language and a very restrictive linker that only allowed very restrictive contexts, but the general shape of the proof is the same as that used in more realistic languages published in research conferences today!</p>
<p>So next time you see a result about a correct (or even hoped to be correct) compiler, ask if it is fully abstract! And if it’s not, are the violations of equivalences something that could be exploited? Or something that would invalidate optimizations?</p>
<p>Some recent conference publications include Devriese et al, <a href="https://lirias.kuleuven.be/bitstream/123456789/570054/2/logrel-for-facomp-authorversion.pdf">Fully-abstract compilation by approximate back-translation</a> published in POPL’16 and New at al, <a href="http://www.ccs.neu.edu/home/amal/papers/fabcc.pdf">Fully Abstract Compilation via Universal Embedding</a>, published in ICFP’16.</p>
<blockquote>
<p>As stated at the top of the post, all the code in this post is available at <a href="https://github.com/dbp/howtoprovefullabstraction" class="uri">https://github.com/dbp/howtoprovefullabstraction</a>. If you have any trouble getting it going, please open an issue on that repository and I’ll help figure it out with you.</p>
</blockquote>
]]></description>
    <pubDate>Thu, 19 Apr 2018 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2018-04-19-how-to-prove-a-compiler-fully-abstract.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>How to prove a compiler correct</title>
    <link>http://dbp.io/essays/2018-01-16-how-to-prove-a-compiler-correct.html</link>
    <description><![CDATA[<h2>How to prove a compiler correct</h2>

<p>by <em>Daniel Patterson</em> on <strong>January 16, 2018</strong></p>

<p>At POPL’18 (Principles of Programming Languages) last week, I ended up talking to <a href="https://anniecherkaev.com">Annie Cherkaev</a> about her really cool DSL (domain specific language) <a href="https://github.com/anniecherk/sweetpea">SweetPea</a> (which she <a href="https://popl18.sigplan.org/event/obt-2018-sweetpea-a-language-for-designing-experiments">presented</a> at Off the Beaten Track 18, a workshop colocated with POPL), which is a “SAT-Sampler aided language for experimental design, targeted for Psychology &amp; Neuroscience”. In particular, we were talking about software engineering, and the work that Annie was doing to test SweetPea and increase her confidence that the implementation is correct!</p>
<p>The topic of how exactly one goes about proving a compiler correct came up, and I realized that I couldn’t think of a high-level (but <em>concrete</em>) overview of what that might look like. Also, like many compilers, hers is implemented in Haskell, so it seemed like a good opportunity to try out the really cool work presented at the colocated conference CPP’18 (Certified Programs and Proofs) titled <a href="https://arxiv.org/abs/1711.09286">“Total Haskell is Reasonable Coq” by Spector-Zabusky, Breitner, Rizkallah, and Weirich</a>. They have a tool (<code>hs-to-coq</code>) that extracts Coq definitions from (certain) terminating Haskell programs (of which at least small compilers hopefully qualify). There are certainly limitations to this approach (see Addendum at the bottom of the page for some discussion), but it seems very promising from an engineering perspective.</p>
<p>The intention of this post is twofold:</p>
<ol style="list-style-type: decimal">
<li>Show how to take a compiler (albeit a tiny one) that was built with no intention of verifying it and after the fact prove it correct. Part of the ability to do this in such a seamless way is the wonderful <code>hs-to-coq</code> tool mentioned above, though there is no reason in principle you couldn’t carry out this translation manually (in practice maintenance becomes an issue, hence realistic verified compilers relying on writing their implementations within theorem provers like Coq and then <em>extracting</em> executable versions automatically, at least in the past – possibly <code>hs-to-coq</code> could change this workflow).</li>
<li>Give a concrete example of proving compiler correctness. By necessity, this is a very simplified scenario without a lot of the subtleties that appear in real verification efforts (e.g., undefined behavior, multiple compiler passes, linking with code after compilation, etc). On the other hand, even this simplified scenario could cover many cases of DSLs, and understanding the subtleties that come up should be much easier once you understand the basic case!</li>
</ol>
<p><strong>The intended audience is: people who know what compilers are (and may have implemented them!) but aren’t sure what it means to prove one correct!</strong></p>
<blockquote>
<p>All the code for this post, along with instructions to get it running, is in the repository <a href="https://github.com/dbp/howtoproveacompiler" class="uri">https://github.com/dbp/howtoproveacompiler</a>. If you have any trouble getting it going, open an issue on that repository.</p>
</blockquote>
<h3 id="dsl-compiler">DSL &amp; Compiler</h3>
<p>To make this simple, my source language is arithmetic expressions with adding, subtraction, and multiplication. I represent this as an explicit data structure in Haskell:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Arith</span> <span class="fu">=</span> <span class="dt">Num</span> <span class="dt">Int</span>
           <span class="fu">|</span> <span class="dt">Plus</span> <span class="dt">Arith</span> <span class="dt">Arith</span>
           <span class="fu">|</span> <span class="dt">Minus</span> <span class="dt">Arith</span> <span class="dt">Arith</span>
           <span class="fu">|</span> <span class="dt">Times</span> <span class="dt">Arith</span> <span class="dt">Arith</span></code></pre></div>
<p>And a program is an <code>Arith</code>. For example, the source expression “1 + (2 * 4)” is represented as <code>Plus 1 (Times 2 4)</code>. The target of this is a sequence of instructions for a stack machine. The idea of the stack machine is that there is a stack of values that can be used by instructions. The target language expressions are:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">StackOp</span> <span class="fu">=</span> <span class="dt">SNum</span> <span class="dt">Int</span>
             <span class="fu">|</span> <span class="dt">SPlus</span>
             <span class="fu">|</span> <span class="dt">SMinus</span>
             <span class="fu">|</span> <span class="dt">STimes</span></code></pre></div>
<p>And a program is a <code>[StackOp]</code>. For example, the previous example “1 + (2 * 4)” could be represented as <code>[SNum 1, SNum 2, SNum 4, STimes, SPlus]</code>. The idea is that a number evaluates to pushing it onto the stack and plus/times evaluate by popping two numbers off the stack and pushing the sum/product respectively back on. But we can make this concrete by writing an <code>eval</code> function that takes an initial stack (which will probably be empty), a program, and either produces an integer (the top of the stack after all the instructions run) or an error (which, for debugging sake, is the state of the stack and rest of the program when it got stuck).</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">eval ::</span> [<span class="dt">Int</span>] <span class="ot">-&gt;</span> [<span class="dt">StackOp</span>] <span class="ot">-&gt;</span> <span class="dt">Either</span> ([<span class="dt">Int</span>], [<span class="dt">StackOp</span>]) <span class="dt">Int</span>
eval (n<span class="fu">:</span>_) []               <span class="fu">=</span> <span class="dt">Right</span> n
eval ns (<span class="dt">SNum</span> n<span class="fu">:</span>xs)         <span class="fu">=</span> eval (n<span class="fu">:</span>ns) xs
eval (n1<span class="fu">:</span>n2<span class="fu">:</span>ns) (<span class="dt">SPlus</span><span class="fu">:</span>xs)  <span class="fu">=</span> eval (n1<span class="fu">+</span>n2<span class="fu">:</span>ns) xs
eval (n1<span class="fu">:</span>n2<span class="fu">:</span>ns) (<span class="dt">SMinus</span><span class="fu">:</span>xs) <span class="fu">=</span> eval (n1<span class="fu">-</span>n2<span class="fu">:</span>ns) xs
eval (n1<span class="fu">:</span>n2<span class="fu">:</span>ns) (<span class="dt">STimes</span><span class="fu">:</span>xs) <span class="fu">=</span> eval (n1<span class="fu">*</span>n2<span class="fu">:</span>ns) xs
eval vals instrs            <span class="fu">=</span> <span class="dt">Left</span> (vals, instrs)</code></pre></div>
<p>Now that we have our source and target language, and know how the target works, we can implement our compiler. Part of why this is a good small example is that the compiler is very simple!</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">compile ::</span> <span class="dt">Arith</span> <span class="ot">-&gt;</span> [<span class="dt">StackOp</span>]
compile (<span class="dt">Num</span> n)       <span class="fu">=</span> [<span class="dt">SNum</span> n]
compile (<span class="dt">Plus</span> a1 a2)  <span class="fu">=</span> compile a2 <span class="fu">++</span> compile a1 <span class="fu">++</span> [<span class="dt">SPlus</span>]
compile (<span class="dt">Minus</span> a1 a2) <span class="fu">=</span> compile a2 <span class="fu">++</span> compile a1 <span class="fu">++</span> [<span class="dt">SMinus</span>]
compile (<span class="dt">Times</span> a1 a2) <span class="fu">=</span> compile a2 <span class="fu">++</span> compile a1 <span class="fu">++</span> [<span class="dt">STimes</span>]</code></pre></div>
<p>The cases for plus/minus/times are the cases that are slightly non-obvious, because they can contain further recursive expressions, but if you think about what the <code>eval</code> function is doing, once the stack machine <em>finishes</em> evaluating everything that <code>a2</code> compiled to, the number that the left branch evaluated to should be on the top of the stack. Then once it finishes evaluating what <code>a1</code> compiles to the number that the right branch evaluated to should be on the top of the stack (the reversal is so that they are in the right order when popped off). This means that evaluating e.g. <code>SPlus</code> will put the sum on the top of the stack, as expected. That’s a pretty informal argument about correctness, but we’ll have a chance to get more formal later.</p>
<h2 id="formalizing">Formalizing</h2>
<p>Now that we have a Haskell compiler, we want to prove it correct! So what do we do? First, we want to convert this to Coq using the <a href="https://github.com/antalsz/hs-to-coq">hs-to-coq</a> tool. There are full instructions at <a href="https://github.com/dbp/howtoproveacompiler" class="uri">https://github.com/dbp/howtoproveacompiler</a>, but the main command that will convert <code>src/Compiler.hs</code> to <code>src/Compiler.v</code>:</p>
<pre><code>STACK_YAML=hs-to-coq/stack.yaml stack exec hs-to-coq -- -o src/ src/Compiler.hs -e hs-to-coq/base/edits</code></pre>
<p>You can now build the Coq code with</p>
<pre><code>make</code></pre>
<p>And open up <code>src/Proofs.v</code> using a Coq interactive mode (I use Proof General within Emacs; with Spacemacs, this is particularly easy: use the <code>coq</code> layer!).</p>
<h2 id="proving-things">Proving things</h2>
<p>We now have a Coq version of our compiler, complete with our evaluation function. So we should be able to write down a theorem that we would like to prove. What should the theorem say? Well, there are various things you could prove, but the most basic theorem in compiler correctness says essentially that running the source program and the target program “does the same thing”. This is often stated as “semantics preservation” and is often formally proven by way of a backwards simulation: whatever the target program does, the source program also should do (for a much more thorough discussion of this, check out William Bowman’s blog post, <a href="https://williamjbowman.com/blog/2017/03/24/what-even-is-compiler-correctness/">What even is compiler correctness?</a>). In languages with ambiguity (nondeterminism, undefined behavior, this becomes much more complicated, but in our setting, we would state it as:</p>
<p><strong>Theorem (informal). For all source arith expressions A, if eval [] (compile A) produces integer N then evaluating A should produce the same number N.</strong></p>
<p>The issue that’s immediately apparent is that we don’t actually have a way of directly evaluating the source expression. The only thing we can do with our source expression is compile it, but if we do that, any statement we get has the behavior of the compiler baked into it (so if the compiler is wrong, we will just be proving stuff about our wrong compiler).</p>
<p>More philosophically, what does it even mean that the compiler is wrong? For it to be wrong, there has to be some external specification (likely, just in our head at this point) about what it was supposed to do, or in this case, about the behavior of the source language that the compiler was supposed to faithfully preserve. To prove things formally, we need to write that behavior down.</p>
<p>So we should add this function to our Haskell source. In a non-trivial DSL, this may be a significant part of the formalization process, but it is also incredibly important, because this is the part where you are actually specifying exactly what the source DSL means (otherwise, the only “meaning” it has is whatever the compiler happens to do, bugs and all). In this example, we can write this function as:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">eval&#39; ::</span> <span class="dt">Arith</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
eval&#39; (<span class="dt">Num</span> n)       <span class="fu">=</span> n
eval&#39; (<span class="dt">Plus</span> a1 a2)  <span class="fu">=</span> (eval&#39; a1) <span class="fu">+</span> (eval&#39; a2)
eval&#39; (<span class="dt">Minus</span> a1 a2) <span class="fu">=</span> (eval&#39; a1) <span class="fu">-</span> (eval&#39; a2)
eval&#39; (<span class="dt">Times</span> a1 a2) <span class="fu">=</span> (eval&#39; a1) <span class="fu">*</span> (eval&#39; a2)</code></pre></div>
<p>And we can re-run <code>hs-to-coq</code> to get it added to our Coq development. We can now formally state the theorem we want to prove as:</p>
<pre><code>Theorem compiler_correctness : forall a : Arith,
  eval nil (compile a) = Data.Either.Right (eval&#39; a).</code></pre>
<p>I’m going to sketch out how this proof went. Proving stuff can be complex, but this maybe gives a sense of some of the thinking that goes into it. To go further, you probably want to take a course if you can find one, or follow a book like:</p>
<ul>
<li>Bertot and Casteron’s <a href="https://archive.org/details/springer_10.1007-978-3-662-07964-5">Coq’Art: Interactive Theorem Proving and Program Development</a>.</li>
<li>Adam Chlipala’s <a href="http://adam.chlipala.net/cpdt/">Certified Programming with Dependent Types</a></li>
<li>Pierce et. al.’s <a href="https://softwarefoundations.cis.upenn.edu/">Software Foundations</a></li>
</ul>
<p>If you were to prove this on paper, you would proceed by induction on the structure of the arithmetic expression, so let’s start that way. The base case goes away trivially and we can expand the case for plus using:</p>
<pre><code>induction a; iauto; simpl.</code></pre>
<p>We see (above the line is assumptions, below what you need to prove):</p>
<pre><code>IHa1 : eval nil (compile a1) = Either.Right (eval&#39; a1)
IHa2 : eval nil (compile a2) = Either.Right (eval&#39; a2)
============================
eval nil (compile a1 ++ compile a2 ++ SPlus :: nil) = Either.Right (eval&#39; a1 + eval&#39; a2)%Z</code></pre>
<p>Which, if we look at it for a little while, we realize two things:</p>
<ol style="list-style-type: decimal">
<li>Our induction hypotheses really aren’t going to work, intuitively because of the <code>Either</code> — our program won’t produce <code>Right</code> results for the subtrees, so there probably won’t be a way to rely on these hypotheses.</li>
<li>On the other hand, what does look like a Lemma we should be able to prove has to do with evaluating a partial program. Rather than trying to induct on the entire statement, we instead try to prove that <code>eval</code>ing a <code>compile</code>d term will result in the <code>eval'</code>d term on the top of the stack. This is an instance of a more general pattern – that often the toplevel statement that you want has too much specificity, and you need to instead prove something that is more general and then use it for the specific case. So here’s (a first attempt) at a Lemma we want to prove:</li>
</ol>
<pre><code>Lemma eval_step : forall a : Arith, forall xs : list StackOp,
        eval nil (compile a ++ xs) = eval (eval&#39; a :: nil) xs.</code></pre>
<p>This is more general, and again we start by inducting on <code>a</code>, expanding and eliminating the base case:</p>
<pre><code>induction a; intros; simpl; iauto.</code></pre>
<p>We now end up with <em>better</em> inductive hypotheses:</p>
<pre><code>IHa1 : forall xs : list StackOp, eval nil (compile a1 ++ xs) = eval (eval&#39; a1 :: nil) xs
IHa2 : forall xs : list StackOp, eval nil (compile a2 ++ xs) = eval (eval&#39; a2 :: nil) xs
============================
eval nil ((compile a1 ++ compile a2 ++ SPlus :: nil) ++ xs) =
eval ((eval&#39; a1 + eval&#39; a2)%Z :: nil) xs</code></pre>
<p>We need to reshuffle the list associativity and then we can rewrite using the first hypotheses:</p>
<pre><code>rewrite List.app_assoc_reverse. rewrite IHa1.</code></pre>
<p>But now there is a problem (this is <em>common</em>, hence going over it!). We want to use our second hypothesis. Once we do that, we can reduce based on the definition of <code>eval</code> and we’ll be done (with this case, but multiplication is the same). The issue is that <code>IHa2</code> needs the stack to be empty, and the stack we now have (since we used <code>IHa1</code>) is <code>eval' a1 :: nil</code>, so it can’t be used:</p>
<pre><code>IHa1 : forall xs : list StackOp, eval nil (compile a1 ++ xs) = eval (eval&#39; a1 :: nil) xs
IHa2 : forall xs : list StackOp, eval nil (compile a2 ++ xs) = eval (eval&#39; a2 :: nil) xs
============================
eval (eval&#39; a1 :: nil) ((compile a2 ++ SPlus :: nil) ++ xs) =
eval ((eval&#39; a1 + eval&#39; a2)%Z :: nil) xs</code></pre>
<p>The solution is to go back to what our Lemma statement said and generalize it now to arbitrary stacks (so in this process we’ve now generalized twice!), so that the inductive hypotheses are correspondingly stronger:</p>
<pre><code>Lemma eval_step : forall a : Arith, forall s : list Num.Int, forall xs : list StackOp,
        eval s (compile a ++ xs) = eval (eval&#39; a :: s) xs.</code></pre>
<p>Now if we start the proof in the same way:</p>
<pre><code>induction a; intros; simpl; iauto.</code></pre>
<p>We run into an odd problem. We have a silly obligation:</p>
<pre><code>match s with
| nil =&gt; eval (i :: s) xs
| (_ :: nil)%list =&gt; eval (i :: s) xs
| (_ :: _ :: _)%list =&gt; eval (i :: s) xs
end = eval (i :: s) xs</code></pre>
<p>Which will go away once we break apart the list <code>s</code> and simplify (if you look carefully, it has the same thing in all three branches of the match). There are (at least) a couple approaches to this:</p>
<ol style="list-style-type: decimal">
<li>We could just do it manually: <code>destruct s; simpl; eauto; destruct s; simpl;    eauto</code>. But it shows up multiple times in the proof, and that’s a mess and someone reading the proof script may be confused what is going on.</li>
<li><p>We could write a tactic for the same thing:</p>
<pre><code>try match goal with
     |[l : list _ |- _ ] =&gt; solve [destruct l; simpl; eauto; destruct l; simpl; eauto]
    end.</code></pre>
This has the advantage that it doesn’t depend on the name, you can call it whenever (it won’t do anything if it isn’t able to discharge the goal), but where to call it is still somewhat messy (as it’ll be in the middle of the proofs). We could hint using this tactic (using <code>Hint Extern</code>) to have it handled automatically, but I generally dislike adding global hints for tactics (unless there is a very good reason!), as it can slow things down and make understanding why proofs worked more difficult.</li>
<li><p>We can also write lemmas for these. There are actually two cases that come up, and both are solved easily:</p>
<pre><code>Lemma list_pointless_split : forall A B:Type, forall l : list A, forall x : B,
    match l with | nil =&gt; x | (_ :: _)%list =&gt; x end = x.
Proof.
  destruct l; eauto.
Qed.
Lemma list_pointless_split&#39; : forall A B:Type, forall l : list A, forall x : B,
        match l with | nil =&gt; x | (_ :: nil)%list =&gt; x | (_ :: _ :: _)%list =&gt; x end = x.
Proof.
  destruct l; intros; eauto. destruct l; eauto.
Qed.</code></pre>
<p>In this style, we can then hint using these lemmas locally to where they are needed.</p></li>
</ol>
<p>Now we know the proof should follow from list associativity, this pointless list splitting, and the inductive hypotheses. We can write this down formally (this relies on the <code>literatecoq</code> library, which is just a few tactics at this point) as:</p>
<pre><code>Lemma eval_step : forall a : Arith, forall s : list Num.Int, forall xs : list StackOp,
        eval s (compile a ++ xs) = eval (eval&#39; a :: s) xs.
Proof.
  hint_rewrite List.app_assoc_reverse.
  hint_rewrite list_pointless_split, list_pointless_split&#39;.

  induction a; intros; simpl; iauto;
    hint_rewrite IHa1, IHa2; iauto&#39;.
Qed.</code></pre>
<p>Which says that we know that we will need the associativity lemma and these list splitting lemmas somewhere. Then we proceed by induction, handle the base case, and then use the inductive hypotheses to handle the rest.</p>
<p>We can then go back to our main theorem, and proceed in a similar style. We prove by induction, relying on the <code>eval_step</code> lemma, and in various places needing to simplify (for the observant reader, <code>iauto</code> and <code>iauto'</code> only differ in that <code>iauto'</code> does a deeper proof search).</p>
<pre><code>Theorem compiler_correctness : forall a : Arith,
  eval nil (compile a) = Data.Either.Right (eval&#39; a).
Proof.
  hint_rewrite eval_step.
  hint_simpl.
  induction a; iauto&#39;.
Qed.</code></pre>
<p>We now have a proof that the compiler that we wrote in Haskell is correct, insofar as it preserves the meaning expressed in the source-level <code>eval'</code> function to the meaning in the <code>eval</code> function in the target. This isn’t, of course, the only theorem you could prove! Another one that would be interesting would be that no compiled program ever got stuck (i.e., never produces a <code>Left</code> error).</p>
<blockquote>
<p>As stated at the top of the post, all the code in this post is available at <a href="https://github.com/dbp/howtoproveacompiler" class="uri">https://github.com/dbp/howtoproveacompiler</a>. If you are looking for more, check out <a href="https://xavierleroy.org/courses/Eugene-2012/">Xavier Leroy’s Oregon Programming Languages Summer School Lectures</a> (videos are <a href="https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html">here</a>, scroll down to find them).</p>
</blockquote>
<p><br/></p>
<p>Thanks to current and former <a href="http://prl.ccs.neu.edu">PRL</a> members, in particular <a href="http://gallium.inria.fr/~scherer/">Gabriel Scherer</a> and <a href="https://williamjbowman.com/">William Bowman</a>, for providing useful feedback on drafts of this post.</p>
<p><br/><br/></p>
<h2 id="addendum-termination">Addendum: termination</h2>
<p>This example was so tiny we haven’t run into something that <em>will</em> be really common: imagine instead of the compile function shown above:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">compile ::</span> <span class="dt">Arith</span> <span class="ot">-&gt;</span> [<span class="dt">StackOp</span>]
compile (<span class="dt">Num</span> n)       <span class="fu">=</span> [<span class="dt">SNum</span> n]
compile (<span class="dt">Plus</span> a1 a2)  <span class="fu">=</span> compile a2 <span class="fu">++</span> compile a1 <span class="fu">++</span> [<span class="dt">SPlus</span>]
compile (<span class="dt">Minus</span> a1 a2) <span class="fu">=</span> compile a2 <span class="fu">++</span> compile a1 <span class="fu">++</span> [<span class="dt">SMinus</span>]
compile (<span class="dt">Times</span> a1 a2) <span class="fu">=</span> compile a2 <span class="fu">++</span> compile a1 <span class="fu">++</span> [<span class="dt">STimes</span>]</code></pre></div>
<p>We instead wanted to take <code>[Arith]</code>. This would still work, and would result in the list of results stored on the stack (so probably you would want to change <code>eval</code> to print everything that was on the stack at the end, not just the top). If you wrote this <code>compile</code>:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">compile ::</span> [<span class="dt">Arith</span>] <span class="ot">-&gt;</span> [<span class="dt">StackOp</span>]
compile []               <span class="fu">=</span> []
compile (<span class="dt">Num</span> n<span class="fu">:</span>xs)       <span class="fu">=</span> <span class="dt">SNum</span> n <span class="fu">:</span> compile xs
compile (<span class="dt">Plus</span> a1 a2<span class="fu">:</span>xs)  <span class="fu">=</span> compile [a2] <span class="fu">++</span> compile [a1] <span class="fu">++</span> <span class="dt">SPlus</span> <span class="fu">:</span> compile xs
compile (<span class="dt">Minus</span> a1 a2<span class="fu">:</span>xs) <span class="fu">=</span> compile [a2] <span class="fu">++</span> compile [a1] <span class="fu">++</span> <span class="dt">SMinus</span> <span class="fu">:</span> compile xs
compile (<span class="dt">Times</span> a1 a2<span class="fu">:</span>xs) <span class="fu">=</span> compile [a2] <span class="fu">++</span> compile [a1] <span class="fu">++</span> <span class="dt">STimes</span> <span class="fu">:</span> compile xs</code></pre></div>
<p>You would get an error when you try to compile the output of <code>hs-to-coq</code>! Coq says that the compile function is not terminating!</p>
<p>This is good introduction into a (major) difference between Haskell and Coq: in Haskell, any term can run forever. For a programming language, this is an inconvenience, as you can end up with code that is perhaps difficult to debug if you didn’t want it to (it’s also useful if you happen to be writing a server that is supposed to run forever!). For a language intended to be used to <em>prove</em> things, this feature would be a non-starter, as it would make the logic unsound. The issue is that in Coq, (at a high level), a type is a theorem and the term that inhabits the type is a proof of that theorem. But in Haskell, you can write:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">anything ::</span> a
anything <span class="fu">=</span> anything</code></pre></div>
<p>i.e., for any type, you can provide a term with that type — that is, the term that simply never returns. If that were possible in Coq, you could prove any theorem, and the entire logic would be useless (or unsound, which technically means you can prove logical falsehood, but since falsehood allows you to prove anything, it’s the same thing).</p>
<p>Returning to this (only slightly contrived) program, it isn’t actually that our program runs forever (and if you do want to prove things about programs that do, you’ll need to do much more work!), just that Coq can’t tell that it doesn’t. In general, it’s not possible to tell this for sufficiently powerful languages (this is what the Halting problem says for Turing machines, and thus holds for anything with similar expressivity). What Coq relies on is that some argument is inductively defined (which we have: both lists and <code>Arith</code> expressions) and that all recursive calls are to <em>structurally smaller</em> parts of the arguments. If that holds, we are guaranteed to terminate, as inductive types cannot be infinite (note: unlike Haskell, Coq is <em>not</em> lazy, which is another difference, but we’ll ignore that). If we look at our recursive call, we called <code>compile</code> with <code>[a1]</code>. While <code>a1</code> is structurally smaller, we put that inside a list and used that instead, which thus violates what Coq was expecting.</p>
<p>There are various ways around this (like adding another argument whose purpose is to track termination, or adding more sophisticated measurements), but there is another option: adding a helper function <code>compile'</code> that does what our original <code>compile</code> did: compiles a single <code>Arith</code>. The intuition that leads to trying this is that in this new <code>compile</code> we are decreasing on both the length of the list and the structure of the <code>Arith</code>, but we are trying to do both at the same time. By separating things out, we can eliminate the issue:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">compile ::</span> [<span class="dt">Arith</span>] <span class="ot">-&gt;</span> [<span class="dt">StackOp</span>]
compile []     <span class="fu">=</span> []
compile (x<span class="fu">:</span>xs) <span class="fu">=</span> compile&#39; x <span class="fu">++</span> compile xs

<span class="ot">compile&#39; ::</span> <span class="dt">Arith</span> <span class="ot">-&gt;</span> [<span class="dt">StackOp</span>]
compile&#39; (<span class="dt">Num</span> n)       <span class="fu">=</span> [<span class="dt">SNum</span> n]
compile&#39; (<span class="dt">Plus</span> a1 a2)  <span class="fu">=</span> compile&#39; a2 <span class="fu">++</span> compile&#39; a1 <span class="fu">++</span> [<span class="dt">SPlus</span>]
compile&#39; (<span class="dt">Minus</span> a1 a2) <span class="fu">=</span> compile&#39; a2 <span class="fu">++</span> compile&#39; a1 <span class="fu">++</span> [<span class="dt">SMinus</span>]
compile&#39; (<span class="dt">Times</span> a1 a2) <span class="fu">=</span> compile&#39; a2 <span class="fu">++</span> compile&#39; a1 <span class="fu">++</span> [<span class="dt">STimes</span>]</code></pre></div>
<h2 id="addendum-do-the-proofs-mean-anything">Addendum: do the proofs mean anything?</h2>
<p>There are limitations to the approach outlined in this post. In particular, what <code>hs-to-coq</code> does is syntactically translate similar constructs from Haskell to Coq, but constructs that have similar syntax don’t necessarily have similar semantics. For example, data types in Haskell are lazy and thus infinite, whereas inductive types in Coq are definitely not infinite. This means that the proofs that you have made are about the version of the program as represented in Coq, not the original program. There are ways to make proofs about the precise semantics of a language (e.g., Arthur Charguéraud’s <a href="http://www.chargueraud.org/softs/cfml/">CFML</a>), but on the other hand, program extraction (which is a core part of verified compilers like <a href="http://compcert.inria.fr/">CompCert</a>) has the same issue that the program being run has been converted via a similar process as <code>hs-to-coq</code> (from Coq to OCaml the distance is less than from Coq to Haskell, but in principle there are similar issues).</p>
<p>And yet, I think that <code>hs-to-coq</code> has a real practical use, in particular when you have an existing Haskell codebase that you want to verify. You likely will need to refactor it to have <code>hs-to-coq</code> work, but that refactoring can be done within Haskell, while the program continues to work (and your existing tests continue to pass, etc). Eventually, once you finish conversion, you may decide that it makes more sense to take the converted version as ground truth (thus, you run <code>hs-to-coq</code> and throw out the original, relying on extraction after that point for an executable), but being able to do this gradual migration (from full Haskell to essentially a Gallina-like dialect of Haskell) seems incredibly valuable.</p>
]]></description>
    <pubDate>Tue, 16 Jan 2018 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2018-01-16-how-to-prove-a-compiler-correct.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>(Cheap) home backups</title>
    <link>http://dbp.io/essays/2018-01-01-home-backups.html</link>
    <description><![CDATA[<h2>(Cheap) home backups</h2>

<p>by <em>Daniel Patterson</em> on <strong>January  1, 2018</strong></p>

<p>Backing things up is important. Some stuff, like code that lives in repositories, may naturally end up in many places, so it perhaps is less important to explicitly back up. Other files, like photos, or personal documents, generally don’t have a natural redundant home, so they need some backup story, and relying on various online services is risky (what if they go out of business, “pivot”, etc), potentially time-consuming to keep track of (services for photos may not allow videos, or at least not full resolution ones, etc), limited in various ways (max file sizes, storage allotments, etc), not to mention bringing up serious privacy concerns. Different people need different things, but what I need, and have built (hence this post describing the system), fulfills the following requirements:</p>
<ol style="list-style-type: decimal">
<li>(Home) scalable – i.e., any reasonable amount of data that I could generate personally I should be able to dump in one place, and be confident that it won’t go away. What makes up the bulk is photos, some music, and some audio and video files. For me, this is currently about 1TB (+-0.5TB).</li>
<li>Cheap. I’m willing to pay about $100-200/year total (including hardware).</li>
<li>Simple. There has to be <em>one</em> place where I can dump files, it has to be simple enough to recover from complete failure of any given piece of hardware even if I haven’t touched it in a long time (because if it is working, I won’t have had to tweak it in months / years). Adding &amp; organizing files should be doable without commandline familiarity, so it can serve my whole home.</li>
<li>Safe. Anything that’s not in my physical control should be encrypted.</li>
<li>Reasonably reliable. Redundancy across hardware, geographic locations, etc. This is obviously balanced with other concerns (in particular, 2 and 3)!</li>
</ol>
<p>I’ve tried various solutions, but what I’ve ended up with seems to be working pretty well (most of it has been running for about a year; some parts are more recent, and a few have been running for much longer). It’s a combination of some cheap hardware, inexpensive cloud storage, and decent backup software.</p>
<h3 id="why-not-an-off-the-shelf-nas">Why not an off-the-shelf NAS?</h3>
<p>In the past, I tried one (it was a Buffalo model). I wasn’t impressed by the software (which was hard to upgrade, install other stuff on it, maintain, etc), the power consumption (this was several years ago, but <em>idle</em> the two-drive system used over 30watts, which is the same power that my similarly aged quad core workstation uses when idle!). Also, a critical element of this system for me is that there is an off-site component, so getting that software on it is extremely important, and I’d rather have a well-supported linux computer to deal with rather than something esoteric. Obviously this depends in the particular NAS you get, but the system below is perfect <em>for me</em>. In particular, setting up and experimenting with the below was much cheaper than dropping hundreds more dollars on a new NAS that may not have worked any better than the old one, and once I had it working, there was certainly no point in going back!</p>
<h3 id="hardware">Hardware</h3>
<ul>
<li><p>$70 - Raspberry Pi 3. This consumes very little power (a little over 1W without the disks, probably around 10W with them spinning, more like 3W when they are idling), takes up very little space, but seems plenty fast enough to act as a file server. That price includes a case, heat-sink, SD card, power adaptor, etc. If you had any of these things, you can probably get a cheaper kit (the single board itself is around $35). Note that you <em>really</em> want a heat-sink on the processor. I ran without it for a while (forgot to install it) and it would overheat and hard lock. It’s a tradeoff that they put a much faster processor in these than in prior generations – I think it’s worth it (it’s an amazingly capably computer for the size/price).</p></li>
<li><p>$75 - Three external USB SATA hard drive enclosures. You might be able to find these cheaper – the ones I got were metal, which seemed good in terms of heat dissipation, and have been running for a little over a year straight without a problem (note: this is actually one more than I’m using at any given time, to make it easier to rotate in new drives; BTRFS, which I’m using, allows you to just physically remove a drive and add a new one, but the preferred method is to have both attached, and issue a <code>replace</code> command. I’m not sure how much this matters, but for $25, I went with the extra enclosure).</p></li>
<li><p>$170 - Two 2TB WD Red SATA drives. These are actually recent upgrades – the server was been running on older 1TB Green drives (four and five years old respectively), but one of them started reporting failures (I would speculate the older of the two, but I didn’t check) so I replaced both. The cheaper blue drives probably would have been fine (the Greens that the Blues have replaced certainly have lasted well enough, running nearly 24/7 for years), but the “intended to run 24/7” Red ones were only $20 more each so I thought I might as well spring for them.</p></li>
</ul>
<h3 id="cloud">Cloud</h3>
<ul>
<li><a href="https://www.backblaze.com/b2/cloud-storage.html">Backblaze B2</a>. This seems to be the cheapest storage that scales down to storing nothing. At my usage (0.5-2TB) it costs about $3-10/month, which is a good amount, and given that it is one of three copies (the other two being on the two hard drives I have attached to the Pi) I’m not worried about the missing reliability vs for example Amazon S3 (B2 gives 8 9s of durability vs S3 at 11 9s, but to get that S3 charges you 3-4x as much).</li>
</ul>
<h3 id="software">Software</h3>
<ul>
<li><p>The Raspberry Pi is running Raspbian (Debian distributed for the Raspberry Pi). This seems to be the best supported Linux distribution, and I’ve used Debian on servers &amp; desktops for maybe 10 years now, so it’s a no-brainer. The external hard drives are a RAID1 with BTRFS. If I were doing it from scratch, I would look into ZFS, but I’ve been migrating this same data over different drives and home servers (on the same file system) since ZFS was essentially totally experimental on Linux, and on Linux, for RAID1, BTRFS seems totally stable (people do not say the same thing about RAID5/6).</p>
<p>The point is, you should use an advanced file system in RAID1 (on ZFS you could go higher, but I prefer simplicity and the power consumption of having just two drives, and can afford to pay for the wasted drive space) that can detect&amp;correct errors, lets you swap in new drives and migrate out old ones, migrate to larger drives, etc. This is essentially the feature-set that both ZFS and BTRFS have, but the former is considered to be more stable and the latter has been in linux for longer.</p></li>
<li><p>For backups, I’m using <a href="https://github.com/gilbertchen/duplicacy">Duplicacy</a>, which is annoyingly similarly named to a much older backup tool called <a href="http://duplicity.nongnu.org/">Duplicity</a> (there also seems to be another tool called <a href="https://github.com/duplicati/duplicati">Duplicati</a>, which I haven’t tried. Couldn’t backup tools get more creative with names? How about calling a tool “albatross”?). It’s also annoyingly <em>not</em> free software, but for personal use, the command-line version (which is the only version that I would be using) <em>is</em> free-as-in-beer. I actually settled on this after trying and failing to use (actually open-source) competitors:</p>
<p>First, I tried the aforementioned <a href="http://duplicity.nongnu.org/">Duplicity</a> (using its friendly frontend <a href="http://duply.net/">duply</a>). I actually was able to make some full backups (the full size of the archive was around 600GB), but then it started erroring out because it would out-of-memory when trying to unpack the file lists. The backup format of Duplicity is not super efficient, but it is very simple (which was appealing – just tar files and various indexes with lists of files). Unfortunately, some operations need memory that seems to scale with the size of the currently backed up archive, which is a non-starter for my little server with 1GB of ram (and in general <em>shouldn’t</em> be acceptable for backup software, but…)</p>
<p>I next tried a newer option, <a href="https://github.com/restic/restic">restic</a>. This has a more efficient backup format, but also had the same problem of running out of memory, though it wasn’t even able to make a backup (though that was probably a good thing, as I wasted less time!). They are aware of it (see, e.g., <a href="https://github.com/restic/restic/issues/450">this issue</a>, so maybe at some point it’ll be an option, but that issue is almost two years old so ho hum…).</p>
<p>So finally I went with the bizarrely sort-of-but-not-really open-source option, Duplicacy. I found other people talking about running it on a Raspberry Pi, and it seemed like the primary place where memory consumption could become a problem was the number of threads used to upload, which thankfully is an argument. I settled on 16 and it seems to work fine (i.e., <code>duplicacy backup -stats -threads 16</code>) – the memory consumption seems to hover below 60%, which leaves a very healthy buffer for anything else that’s going on (or periodic little jumps), and regardless, more threads don’t seem to get it to work faster.</p>
<p>The documentation on how to use the command-line version is a little sparse (there is a GUI version that costs money), but once I figured out that to configure it to connect automatically to my B2 account I needed a file <code>.duplicacy/preferences</code> that looked like (see <code>keys</code> section; the rest will probably be written out for you if you run <code>duplicacy</code> first; alternatively, just put this file in place and everything will be set up):</p>
<pre><code>[
  {
      &quot;name&quot;: &quot;default&quot;,
      &quot;id&quot;: &quot;SOME-ID&quot;,
      &quot;storage&quot;: &quot;b2://BUCKET_NAME&quot;,
      &quot;encrypted&quot;: true,
      &quot;no_backup&quot;: false,
      &quot;no_restore&quot;: false,
      &quot;no_save_password&quot;: false,
      &quot;keys&quot;: {
          &quot;b2_id&quot;: &quot;ACCOUNT_ID&quot;,
          &quot;b2_key&quot;: &quot;ACCOUNT_KEY&quot;,
          &quot;password&quot;: &quot;ENCRYPTION_PASSWORD&quot;
      }
  }
]</code></pre>
<p>Everything else was pretty much smooth sailing (though, as per usual, the initial backup is quite slow. The Raspberry Pi 3 processor is certainly much faster than previous Raspberry Pis, and fast enough for this purpose, but it definitely still has to work hard! And my residential cable upstream is not all that impressive. After a couple days though, the initial backup will complete!).</p>
<p>Periodic backups run with the same command, and intermediate ones can be pruned away as well (I use <code>duplicacy prune -keep 30:180 -keep 7:30 -keep 1:1</code>, run after my daily backup, to keep monthly backups beyond 6 months, weekly beyond 1 month, and daily below that. I have a cron job that runs the backup daily, so the last is not strictly necessary, but if I do manual backups it’ll clean them up over time. Since I pretty much never delete files that are put into this archive, pruning isn’t really about saving space, as barring some error on the server the latest backup should contain every file, but it is nice to have the list of snapshots be more manageable).</p>
<p>To restore from total loss of the Pi, you just need to put the config file above into <code>.duplicacy/preferences</code> relative to the current directory on any machine and you can run <code>duplicacy restore</code>. You can also grab individual files (which I tested on a different machine; I haven’t tested restoring a full backup) by creating the above mentioned file and then running <code>duplicacy list -files -r N</code> (where N is the snapshot you want to get the file from; run <code>duplicacy list</code> to find which one you want) and then to get a file <code>duplicacy cat -r N path/to/file &gt; where/to/put/it</code>.</p></li>
<li><p>I’m still working out how to detect errors in the hard drives automatically. I can see them manually by running <code>sudo btrfs device stats /mntpoint</code> (which I do periodically). When this shows that a drive is failing (i.e., read/write errors), add a new drive to the spare enclosure, format it, and then run <code>sudo   btrfs replace start -f N /dev/sdX /mntpoint</code> where N is the number of the device that is failing (when you run <code>sudo btrfs fi show /mntpoint</code>) and <code>/dev/sdX</code> is the new drive. To check for and correct errors in the file system (not the underlying drive), run <code>sudo btrfs scrub start /mntpoint</code>. This will run in the background; if you care you can check the status with <code>sudo btrfs scrub status /mntpoint</code>. Based on recommendations, I have the scrub process run monthly via a cron job.</p></li>
<li><p>If you want to expand the capacity of the disks, replace the drives as if they failed (see previous bullet) and then run <code>sudo btrfs fi resize N:max   /mntpoint</code> for each <code>N</code> (run <code>sudo btrfs fi show</code> to see what your dev ids are). When you replace them, they stay at the same capacity – this resize expands the filesystem to the full device. As I mentioned earlier, I did this to replace 1TB WD Green drives with 2TB WD Red drives (so I replaced one, then the next, then did the resize on both).</p></li>
<li><p>For tech people (i.e., who are comfortable with <code>scp</code>), this setup is enough – just get files onto the server, into the right directory, and it’ll be all set. For less tech-savvy people, you can install samba on the raspberry pi and then set up a share like the following (put this at the bottom of <code>/etc/samba/smb.conf</code>):</p>
<pre><code>[sharename]
comment = Descriptive name
path = /mntpoint
browseable = yes
writeable = yes
read only = no
only guest = no
create mask = 0777
directory mask = 0777
public = yes
guest ok = no</code></pre>
<p>Then set <code>pi</code>s password with <code>sudo smbpasswd -i pi</code>. Now restart the service with <code>sudo /etc/init.d/sambda restart</code> and then from a mac (and probably windows; not sure how as I don’t have any in my house) you can connect to the pi with the “Connect to Server” interface, connect as the <code>pi</code> user with the password you set, and see the share. Note that to be able to make changes, the <code>/mntpoint</code> (and what’s in it) needs to be writeable by the <code>pi</code> user. You can also use a different user, set up samba differently, etc.</p></li>
</ul>
<h3 id="summary">Summary</h3>
<p>The system described above runs 24/7 in my home. It cost $325 in hardware (which, if you want to skip the extra USB enclosure to start and use WD Blue drives rather than Red ones you can cut $65 – i.e., $260 total), $1/month in electricity (I haven’t measured this carefully, but that’s what 10W costs where I live) and currently costs about $3/month in cloud storage, though that will go up over time, so to be more fair let’s say $5/month. Assuming no hardware replacements for three years (which is the warrantee on the hard drives I have, so a decent estimate), the total cost over that time is $325 + $54 + $170 = $549, or around $180 per year, which is squarely in the range that I wanted.</p>
]]></description>
    <pubDate>Mon, 01 Jan 2018 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2018-01-01-home-backups.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>Why test in Haskell?</title>
    <link>http://dbp.io/essays/2014-10-05-why-test-in-haskell.html</link>
    <description><![CDATA[<h2>Why test in Haskell?</h2>

<p>by <em>Daniel Patterson</em> on <strong>October  5, 2014</strong></p>

<p>Every so often, the question comes up, should you test in Haskell, and if so, how should you do it?</p>
<p>Most people agree that you should test pure, especially complicated, algorithmic code. Quickcheck<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> is a great way to do this, and most Haskellers have internalized this (Quickcheck was invented here, so it must provide value!). What’s less clear (or at least, more debated!) is whether you should be testing monadic code, glue code, and code that just isn’t all that complicated.</p>
<h2 id="quickcheck">Quickcheck?</h2>
<p>A lot Haskell I’m writing these days is with the web framework Snap, and web handlers often have the type <code>Handler App App ()</code> - where <code>Handler</code> is a web monad (giving access to request information, and the ability to write response data), and <code>App</code> indicates access to application specific state (like database connections, templates, etc).</p>
<p>So the inputs (ie, how to run this action) include any HTTP request and any application state, and the only outputs are side effects (as all it returns is unit). Using Quickcheck here is… challenging. You could restrict the generated requests to have the right URL, and even have the right query parameters, but since the query parameters are just text, if they were supposed to be more structured (like an interger), the chance of actually generating text that was just a number is pretty low… And then if the number were supposed to be the id of an element in the database….</p>
<p>But assume that we restrict it so that it’s only generating ids for elements in the database, what are the properties we are asserting? Let’s say that the handler looked up the element, and rendered it on the page. So then we want to assert something about the content of the response (which is wrapped up in the <code>Handler</code> monad). But maybe it should also increment a view count in the database. And assuming that we wrote all these into properties, what are the elements in the database that it is choosing among? And in some senses we’ve now restricted too much, because we may want to see what the behavior is like for slightly invalid inputs. Say, integer id’s that don’t correspond to elements in the database. This is all certainly possible, and may be worth doing, but it seems pretty difficult. Which is totally different from the experience of testing nice pure functions!</p>
<p>Let’s try to tease out a little bit of why testing this kind of code with Quickcheck is hard. One problem is that the input space, as determined by the type, is massive. And for most of the possible inputs, the result should be some version of a no-op. Another problem is the dependence on state, as the possible inputs are contingent on external state, and the outputs are primarily changes to state, each of which, again, is a massive space.</p>
<p>But having massive input and output spaces is not necessarily a reason not to be using randomized testing. Indeed, this is exactly the kind of thing that fuzz-testing of web browsers, for example, has done with great effect<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. The problem in this case is that the size of the input and output space is not at all in proportion to the complexity of the code. If we were writing an HTTP server, we may indeed want to be generating random requests, throwing them at the web server, and making sure it was generating well-formed responses (404s being perfectly fine).</p>
<h2 id="not-that-complicated">Not that complicated…</h2>
<p>But we’re just writing a little bit of glue code. Which isn’t that complicated. And can be tested manually pretty easily. And may change rapidly.</p>
<p>Which means spending a lot of time setting up property based tests (which in these sorts of cases are necessarily going to be quite a bit more complicated than quintessential Quickcheck examples like showing that <code>reverse . reverse = id</code>).</p>
<p>But you’re still writing code that has types that massively underspecify it’s behavior. Which should make you nervous, at least a little. Now granted, you should keep that underspecified code as thin as possible - validate the query parameters, the URL, etc, and then call a function with a type that much more clearly specifies what it is supposed to do. For example (this is coming from Snap code, with some details ellided, but should be reasonably easy to understand):</p>
<pre><code>f :: Handler App App ()
f = route [(&quot;/foo/:id&quot;, do i &lt;- read &lt;$&gt; getParam &quot;id&quot;
                           res &lt;- lookupAndRenderFoo (FooId i)
                           writeText res)]

lookupAndRenderFoo :: FooId -&gt; Handler App App Text
lookupAndRenderFoo = undefined</code></pre>
<p>And certainly, this is a good pattern to use. We went from a function that had as input space any HTTP request (and any application specific state), and as output any HTTP response (as well as any side effects in the <code>Handler</code> monad) and split it into two functions. One still has the same input and output as before, but is very short, and the other is a function with input the id of a specific element, and as output <code>Text</code>, but still can perform any side effects in and read any data from within the <code>Handler</code> monad.</p>
<h2 id="increasing-complexity">Increasing complexity?</h2>
<p>We could split that further, and write a function with type <code>Foo -&gt; Text</code>, but we would start getting in our own way, as if we wanted to render with a template, the templates exist within the context of the <code>Handler</code> monad, so we would have to look up a template first, and we would have ended up creating many new functions, as well as a bit of extra complexity, all for the sake of splitting our code up into layers, where the last one is pure and easy to test (the rest still have all the same problems).</p>
<p>Depending on how complex that last layer is, this may totally be worth it. If your code is dealing with human lives or livelihoods, by all means, isolate that code into as small a portion as possible and test the hell out of it. But it makes coding harder, and makes you move slower. And if you want to change the logic, you may now have to change many different functions, instead of just one.</p>
<p>Which is where we come to the argument that testing slows things down, and that for rapidly changing code, it just doesn’t matter.</p>
<h2 id="what-about-just-not-sampling">What about just not sampling?</h2>
<p>But if we step back a bit, we realize that what Quickcheck is trying to do is to sample representatively (well, with a bias towards edge cases) over the type of the input. And it’s easy to see why that’s appealing, as it gives you reasonable confidence that any use of the function behaves as desired. But if we forget about that, as we already know that our types completely underspecify the behavior, we realize all that we really care about is that the code does what we think it should do on a few example cases. That’s what we were going to manually verify after writing the code anyway.</p>
<p>Which is easy to test. With Snap, I’d write some tests for the above snippet like<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>:</p>
<pre><code>do f &lt;- create ()
   let i = show . unFooId . fooId $ f
   get (&quot;/foo/&quot; ++ i) &gt;&gt;= should200
   get (&quot;/foo/&quot; ++ i) &gt;&gt;= shouldHaveText (fooDescription f)
   get (&quot;/foo/&quot; ++ show (1 + i)) &gt;&gt;= should404</code></pre>
<p>And call it a day. This misses vasts swaths of inputs, and asserts very little about the outputs, but it also tells you a huge amount more about the correctness of the code than the fact that it typechecked did. And as you iterate and refactor your application, you get the assurance that this handler:</p>
<ol style="list-style-type: decimal">
<li>still exists.</li>
<li>still looks up the element from the database.</li>
<li>still puts the description somewhere on the page.</li>
<li>doesn’t work for ids that don’t correspond to elements in the database.</li>
</ol>
<p>Which seems like a lot of assurance for a very small amount of work. And if your application is fast moving, this benefits you even more, as the faster you move, the more likely you are to break things (at least, that’s always been my experience!). If you do decide to rewrite this handler, fixing these tests is going to take a tiny amount of time (probably less time than you spend manually confirming that the change worked).</p>
<h2 id="why-this-should-be-expected-to-work.">Why this should be expected to work.</h2>
<p>To take it a little further, and perhaps justify from a somewhat theoretical point of view why these sorts of tests are so valuable, consider all possible implementations of any function (or monadic action). The possible implementations with the given type are a subset of all the possible implementations, but still potentially a pretty large one (our example of a web handler certainly has this property).</p>
<p>This perspective gives us some intuition on why it is much easier to test simple, pure functions. There are only four possible implementations of a <code>Bool -&gt; Bool</code> function, so testing <code>not</code> via sampling seems pretty tractable. To go even further, we get into the territory of “Theorems for Free”<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>, where there is only one implementation for an <code>(a,b) -&gt; a</code> function, so testing <code>fst</code> is pointless.</p>
<p>But returning to our case of massive spaces of well-typed implementations: A single test, like one of the above, corresponds to another subset of all the possible implementations. For example, the first test corresponds to the subset that return success when passed the given url via GET request. Since we’re in Haskell, we also get a guarantee that the set of implementations that fulfill the test is a (non)strict subset of the set of implementations that fulfill the type, as if this were not the case, our test case wouldn’t type check. The problem with the first test, of course, is that there are all sorts of bogus implementations that fulfill it. For example, the handler that always returns success would match that test.</p>
<p>But even still, it <em>is</em> a strict subset of the implementations that fulfill the type (for example, the handler that always returns 404 is not in this set), so we’re guaranteed to have improved the chance that our code is correct, even with such a weak test (granted, it actually may not be that weak of a test - in one project, I have a menu generated from a data structure in code, and a test that iterates through all elements of the menu, checking that hitting each url results in a 200. And this has caught many refactoring problems!).</p>
<p>Where we really start to benefit is as we add a few more tests. The second test shows that the handler must somehow get an element out of the database (provided our <code>create ()</code> test function is creating relatively unique field names), which is <em>another</em> (strict) subset of the set of implementations that fulfill the type. And we now know that our implementation must be somewhere in the intersection of these two subsets.</p>
<p>It shouldn’t be hard to convince yourself that through the process of just writing a few (well chosen) tests you can vastly reduce the possibility of writing incorrect implementations. Which, when we are writing relatively straightforward code, will probably be good enough to ensure that the code is actually correct. And will continue to verify that as the code evolves. Pretty good for a couple lines of code.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For those who haven’t used Quickcheck, it allows you to specify properties that a function should satisfy, and possibly a way to generate random values of the input type (if your input is a standard type, it already knows how to do this), and it will generate some number of inputs and verify that the property holds for all of them.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>See, for example, <a href="http://www.squarefree.com/2014/02/03/fuzzers-love-assertions/">www.squarefree.com/2014/02/03/fuzzers-love-assertions/</a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This syntax is based on the <a href="http://hackage.haskell.org/package/hspec-snap">hspec-snap</a> package, which I chose because I’m familiar with it (and wrote it). The <code>create</code> line is from some not-yet-integrated-or-released, at least at time of publishing, work to add factory support to the library (sorry!). With that said, the advice should hold no matter what you’re doing.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>See Wadler’s “Theorems for Free”, 1989.<a href="#fnref4">↩</a></p></li>
</ol>
</div>
]]></description>
    <pubDate>Sun, 05 Oct 2014 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2014-10-05-why-test-in-haskell.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>A Hacker's Replacement for GMail</title>
    <link>http://dbp.io/essays/2013-06-29-hackers-replacement-for-gmail.html</link>
    <description><![CDATA[<h2>A Hacker's Replacement for GMail</h2>

<p>by <em>Daniel Patterson</em> on <strong>June 29, 2013</strong></p>

<p><em>Note: Since writing this I’ve replaced Exim with Postfix and Courier with Dovecot. This is outlined in the Addendum, but the main text is unchanged. Please read the whole guide before starting, as you can skip some of the steps and go straight to the final system.</em></p>
<h2 id="motivation">Motivation</h2>
<p>I reluctantly switched to GMail about six months ago, after using many so-called “replacements for GMail” (the last of which was Fastmail). All of them were missing one or more features that I require of email:</p>
<ol style="list-style-type: decimal">
<li>Access to the same email on multiple machines (but, these can all be machines I control).</li>
<li>Access to important email on my phone (Android). Sophisticated access not important - just a high-tech pager.</li>
<li>Ability to organize messages by threads.</li>
<li>Ability to categorize messages by tags (folders are <em>not</em> sufficient).</li>
<li>Good search functionality.</li>
</ol>
<p>But, while GMail has all of these things, there were nagging reasons why I still wanted an alternative: handing an advertising company most of my personal and professional correspondance seems like a bad idea, having no (meaningful) way to either sign or encrypt email is unfortunate, and while it isn’t a true deal-breaker, having lightweight programmatic access to my email is a really nice thing (you can get a really rough approximation of this with the RSS feeds GMail provides). Furthermore, I’d be happy if I only get important email on my phone (ie, I want a whitelist on the phone - unexpected email is not something that I need to respond to all the time, and this allows me to elevate the notification for these messages, as they truly are important).</p>
<p>Over the past several months, I gradually put together a mail system that provides all the required features, as well as the three bonuses (encryption, easy programmatic access, and phone whitelisting). I’m describing it as a “Hacker’s Replacement for GMail” as opposed to just a “Replacement for GMail” because it involves a good deal of familiarity with Unix (or at least, to set up and debug the whole system it did. Perhaps following along is easier). But, the end result is powerful enough that for me, it is worth it. I finally switched over to using it primarily recently, confirming that all works as expected. I wanted to share the instructions in case they prove useful to someone else setting up a similar system.</p>
<p>This is somewhere between an outline and a HOWTO. I’ve organized it roughly in order of how I set things up, but some of the parts are more sketches than detailed instructions - supplement it with normal documentation. Most are based on notes from things as I did them, only a few parts were reconstructed. In general, I try to highlight the parts that were difficult / undocumented, and gloss over stuff that should be easy (and/or point to detailed docs). Without further ado:</p>
<h2 id="overall-design">Overall Design</h2>
<ul>
<li>Debian GNU/Linux as mail server operating system (both Linux and Mac as clients, though Windows should be doable)</li>
<li>Exim4 as the mail server</li>
<li>Courier-IMAP for mobile usage</li>
<li>Spamassassin (with Pyzor) for spam</li>
<li><a href="http://notmuchmail.org/">notmuch</a> to manage the email database+tags+search</li>
<li><a href="https://github.com/teythoon/afew">afew</a> for managing notmuch tagging/email moving</li>
<li>Emacs client for notmuch on all computers</li>
<li><a href="https://play.google.com/store/apps/details?id=com.fsck.k9">K9-Mail</a> on android (my phone)</li>
</ul>
<p>Mail is received by the mail server and put in a Archive subdirectory which is <em>not</em> configured for push in K9-Mail. The mail is processed and tagged by afew, and any messages with the tag “important” are moved into the Important subdirectory. This directory is set up for push in K9-Mail, so I get all important email right away. No further tagging can be done through the mobile device, but that wasn’t a requirement. read/unread status will be synced two-way to notmuch, which <em>is</em> important.</p>
<h2 id="step-by-step-instructions">Step By Step Instructions</h2>
<ol style="list-style-type: decimal">
<li><p>The first and most important part is having a server. I’ve been really happy with VPSes I have from <a href="https://www.digitalocean.com/?refcode=93aab578a407">Digital Ocean</a> (warning: that’s a referral link. <a href="https://www.digitalocean.com">Here’s one without.</a>) - they provide big-enough VPSes for email and a simple website for $5/month. There are also many other providers. The important thing is to get a server, if you don’t already have one.</p></li>
<li><p>The next thing you’ll need is a domain name. You can use a subdomain of one you already have, but the simplest thing is to just get a new one. This is $10-15/year. Once you have it, you want to set a few records (these are set in the “Zone File”, and should be easy to set up through the online control panel of whatever registrar you used):</p></li>
</ol>
<pre><code>A mydomain.com. IP.ADDR.OF.SERVER (mydomain.com. might be written @)
MX 10 mydomain.com.</code></pre>
<p>This sets the domain to point to your server, and sets the mail record to point to that domain name. You will also need to set up a PTR record, or reverse DNS. If you got the server through Digital Ocean, you can set up the DNS records through them, and they allow you to set the PTR record for each server easily. Whereever you set it up, it should point at mydomain.com. (Note trailing period. Otherwise it will resolve to mydomain.com.mydomain.com - not what you want!).</p>
<ol start="3" style="list-style-type: decimal">
<li>Now set up the mail server itself. I use Debian, but it shouldn’t be terribly different with other distributions (but you should follow their instructions, not the ones I link to here, because I’m sure there are specifics that are dependent on how Debian sets things up). Since Debian uses Exim4 by default, I used that, and set up Courier as an IMAP server. I followed these instructions: <a href="http://blog.edseek.com/~jasonb/articles/exim4_courier/">blog.edseek.com/~jasonb/articles/exim4_courier/</a> (sections 2, 3, and 4). The only important thing I had to change was to force the hostname, by finding the line it <code>/etc/exim4/exim4.conf.template</code> that looks like:</li>
</ol>
<pre><code>.ifdef MAIN_HARDCODE_PRIMARY_HOSTNAME</code></pre>
<p>And adding above it, <code>MAIN_HARDCODE_PRIMARY_HOSTNAME = mydomain.com</code> (no trailing period). This is so that the header that the mail server displays matches the domain. If this isn’t the case, some mail servers won’t deliver messages. At this point, you can test the mail server by sending yourself emails, using the <code>swaks</code> tool, or running it through an online testing tool like <a href="http://mxtoolbox.com/">MX Toolbox</a></p>
<p>The last important thing is to set up spam filtering. When using a big email provider that spends a lot of effort filtering spam (and has huge data sets to do it), it’s easy to forget how much spam is actually sent. But, fortunately open source software is also capable of eliminating it. To set Spamassassin up, I generally followed the documentation on <a href="http://wiki.debian.org/Exim#Spam_scanning">the debian wiki</a>. I changed the last part of the configuration so that instead of changing the subject for spam messages to have “<code>***SPAM***</code>”, it adds the following header:</p>
<pre><code>add_header = X-Spam-Flag: YES</code></pre>
<p>This is the header that the default spam filter from <code>afew</code> will look for and tag messages as spam with. Once messages are tagged as spam, they won’t show up in searches, won’t ever end up in your inbox, etc. On the other hand, they aren’t ever deleted, so if something does end up there, you can always find it (you just have to use notmuch search with the <code>--exclude=false</code> parameter).</p>
<p>That sets up basic Spamassassin, which works quite well. To make it work even better, we’ll install <a href="http://wiki.apache.org/spamassassin/UsingPyzor">Pyzor</a>, which is a service for collaborative spam filtering (sort of an open source system that gets you similar behavior to what GMail can do by having access to so many people’s email). It works by constructing a digest of the message and hashing it, and then sending that hash to a server to see if anyone has marked it as spam.</p>
<p>Install pyzor with <code>aptitude install pyzor</code>, then run <code>pyzor discover</code> (as root), and at least on my system, I needed to run <code>chmod a+r /etc/mail/spamassassin/servers</code> (as root) in order to have it work (the following test command would report permission denied on that file if I didn’t). Now restart spamassassin (<code>/etc/init.d/spamassassin restart</code>) and test that it’s working, by running:</p>
<pre><code>echo &quot;test&quot; | spamassassin -D pyzor 2&gt;&amp;1 | less</code></pre>
<p>This should print (among other things):</p>
<pre><code>Jun 29 16:31:53.026 [24982] dbg: pyzor: network tests on, attempting Pyzor
Jun 29 16:31:54.640 [24982] dbg: pyzor: pyzor is available: /usr/bin/pyzor
Jun 29 16:31:54.641 [24982] dbg: pyzor: opening pipe: /usr/bin/pyzor --homedir ...
Jun 29 16:31:54.674 [24982] dbg: pyzor: [25043] finished: exit 1
Jun 29 16:31:54.674 [24982] dbg: pyzor: check failed: no response</code></pre>
<p>According to <a href="http://wiki.apache.org/spamassassin/UsingPyzor">the documentation</a>, this is expected, because “test” is not a valid message.</p>
<ol start="4" style="list-style-type: decimal">
<li>Now we want to set up our delivery. Create a <code>.forward</code> file in the home directory of the account on the server that is going to recieve mail. It should contain</li>
</ol>
<pre><code># Exim filter

save Maildir/.Archive/</code></pre>
<p>What this does is put all mail that is recieved into the Archive subdirectory (the dots are convention of the version of the Maildir format that Courier-IMAP uses).</p>
<ol start="5" style="list-style-type: decimal">
<li>Next, we want to set up notmuch. You can install it and the python bindings (needed by afew) with:</li>
</ol>
<pre><code>aptitude install notmuch python-notmuch</code></pre>
<ol start="6" style="list-style-type: decimal">
<li><p>Run <code>notmuch setup</code> and put in your name, email, and make sure that the directory to your email archive is “/home/YOURUSER/Maildir”. Run <code>notmuch new</code> to have it create the directories and, if you tested the mail server by sending yourself messages, import those initial messages.</p></li>
<li><p>Install afew from <a href="https://github.com/teythoon/afew">github.com/teythoon/afew</a>. You can start with the default configuration, and then add filters that will add the tag ‘important’, as well as any other automatic tagging you want to have. I commented out the ClassifyingFilter because it wasn’t working - and I wasn’t convinced I wanted it, so didn’t bother to figure out how te get it to work.</p></li>
</ol>
<p>Some simple filters look like:</p>
<pre><code>[Filter.0]
message = messages from someone
query = from:someone.important@email.com
tags = +important
[Filter.1]
message = messages I don&#39;t care about
query = subject:Deal
tags = -unread +deals</code></pre>
<p>For the <code>[MailMover]</code> section, you want the configuration to look like:</p>
<pre><code>[MailMover]
folders = Archive Important
max_age = 15

# rules
Archive = &#39;tag:important AND NOT tag:spam&#39;:.Important
Important = &#39;NOT tag:important&#39;:.Archive &#39;tag:spam&#39;:.Archive</code></pre>
<p>This says to take anything in Archive with the important tag and put it in important (but never spam). Note that the folders we are moving to are prefixed with a dot, but the names of the folders aren’t. Now we need to set everything up to run automatically.</p>
<ol start="8" style="list-style-type: decimal">
<li>We are going to use inotify, and specifically the tool <code>incron</code>, to watch for changes in our .Archive inbox and add files to the database, tag them, and move those that should be moved to .Important. On Debian, you can obtain <code>incron</code> with:</li>
</ol>
<pre><code>aptitude install incron</code></pre>
<p>Now edit your incrontab (similar to crontab) with <code>incrontab -e</code> and put an entry like:</p>
<pre><code>/home/MYUSER/Maildir/.Archive/new IN_MOVED_TO,IN_NO_LOOP /home/MYUSER/bin/my-notmuch-new.sh</code></pre>
<p>This says that we want to watch for <code>IN_MOVED_TO</code> events, we don’t want to listen while the script is running (if something goes wrong with your importing script, you could cause an infinite spawning of processes, which will take down the server). If a message is delivered while the script is running, it might not get picked up until the next run, but for me that was fine (you may want to eliminate the <code>IN_NO_LOOP</code> option and see if it actually causes loops. In previous configurations, I crashed my server twice through process spawning loops, and didn’t want to do it again while debugging). When <code>IN_MOVED_TO</code> occurs, we call a script we’ve written. You can obviously put this anywhere, just make it executable:</p>
<pre><code>#!/bin/bash
/usr/local/bin/notmuch new &gt;&gt; /dev/null 2&gt;&amp;1
/usr/local/bin/afew -nt &gt;&gt; /dev/null 2&gt;&amp;1
/usr/local/bin/afew -m &gt;&gt; /dev/null 2&gt;&amp;1</code></pre>
<p>It is intentionally being very quiet because output from cron jobs will trigger emails… and thus if there were a mistake, we could be in infinite loop land again. This means you should make sure the commands are working (ie, there aren’t mistakes in your config files), because you won’t see any debug output from them when they are run through this script.</p>
<ol start="8" style="list-style-type: decimal">
<li>Now let’s set up the mobile client. I’m not sure of a good way to do this on iOS (aside from just manually checking the Important folder), but perhaps a motivated person could figure it out. Since I have an Android phone, it wasn’t an issue. On Android, install K9-Mail, and set up your account with the incoming / outgoing mail server to be just ‘mydomain.com’. Click on the account, and it will show just Inbox (not helpful). Hit the menu button, then click folders, and check “display all folders”. Now hit the menu again and click folders and hit “refresh folders”.</li>
</ol>
<p>Provided at least one message has been put into Important and Archive, those should both show up now. Open the folder ‘Important’ and use the settings to enable push for it. Also add it to the Unified Inbox. Similarly, disable push on the Inbox (this latter doesn’t really matter, because we never deliver messages to the inbox). If you have trouble finding these settings (which I did for a while), note that the settings that are available are contingent upon the screen you are on. The folders settings only exist when you are looking at the list of folders (not the unified inbox / list of accounts, and not the contents of a folder).</p>
<ol start="9" style="list-style-type: decimal">
<li>Finally, the desktop client. I’m using the emacs client, because I spend most of my time inside emacs, but there are several other clients - one for vim, one called ‘bower’ that is curses based (that I’ve used before, but is less featureful than the emacs one), and a few others. <code>alot</code>, a python client, won’t work, because it assumes that the notmuch database is local (which is a really stupid assumption). The rest just assume that <code>notmuch</code> is in the path. This means that you can follow the instructions here: <a href="http://notmuchmail.org/remoteusage/">notmuchmail.org/remoteusage</a> to have the desktop use the mail database on the server. To test, run <code>notmuch count</code> on your local machine, and it should return the same thing (the total number of messages) as it does on the mail server.</li>
</ol>
<p>Once this is working, install notmuch locally, so that you get the emacs bindings (or, just download the source and put the contents of the emacs folder somewhere and include it in your .emacs). You should now be able to run <code>M-x notmuch</code> in emacs and get to your inbox. Setting up mail sending is a little trickier - most of the documentation I found didn’t work!</p>
<p>The first thing to do, in case your ISP is like mine and blocks port 25, is to change the default listening port for the server. Open up <code>/etc/default/exim4</code> and set <code>SMTPLISTENEROPTIONS</code> equal to <code>-oX 25:587 -oP /var/run/exim4/exim.pid</code>. This will have it listen on both 25 and 587.</p>
<p>Next, set up emacs to use your mail server to send mail, and to load notmuch. This incantation in your <code>.emacs</code> should do the trick:</p>
<pre><code>;; If you opted to just stick the elisp files somewhere, add that path here:
;; (add-to-list &#39;load-path &quot;~/path/folder/with/emacs-notmuch&quot;)
(require &#39;notmuch)
(setq smtpmail-starttls-credentials &#39;((&quot;mydomain.com&quot; 587 nil nil))
      smtpmail-auth-credentials (expand-file-name &quot;~/.authinfo&quot;)
      smtpmail-default-smtp-server &quot;mydomain.com&quot;
      smtpmail-smtp-server &quot;mydomain.com&quot;
      smtpmail-smtp-service 587)
(require &#39;smtpmail)
(setq message-send-mail-function &#39;smtpmail-send-it)
(require &#39;starttls)</code></pre>
<p>Now eval your .emacs (or restart emacs), and you are almost ready to send mail.</p>
<p>You just need to put a line like this into <code>~/.authinfo</code>:</p>
<pre><code>machine mydomain.com login MYUSERNAME password MYPASSWORD port 587</code></pre>
<p>With appropriate permissions (<code>chmod 600 ~/.authinfo</code>).</p>
<p>Now you can test this by typing <code>C-x m</code> or <code>M-x notmuch</code> and then from there, hit the ‘m’ key - both of these open the composition window. Type a message and who it is to, and then type <code>C-c C-c</code> to send it. It should take a second and then say it was sent at the bottom of the window.</p>
<p>This should work as-is on Linux. Another machine I sometimes use is a mac, and things are a little more complicated. The main problem is that to send mail, we need starttls. You can install <code>gnutls</code> through Homebrew, Fink, or Macports, but the next problem is that if you are using Emacs installed from emacsformacosx.com (and thus it is a graphical application), it is not started from a shell, which means it doesn’t have the same path, and thus doesn’t know how to find gnutls. To fix this problem (which is more general), you can install a tiny Emacs package called <code>exec-path-from-shell</code> (this requires Emacs 24, which you should use - then <code>M-x package-install</code>) that interrogates a shell about what the path should be. Then, we just have to tell it to use <code>gnutls</code> and all should work. We can do this all in a platform specific way (so it won’t run on other platforms):</p>
<pre><code>(when (memq window-system &#39;(mac ns))
  (exec-path-from-shell-initialize)
  (setq starttls-use-gnutls t)
  (setq starttls-gnutls-program &quot;gnutls-cli&quot;)
  (setq starttls-extra-arguments nil)
  )</code></pre>
<ol start="10" style="list-style-type: decimal">
<li>Address lookup. It’s really nice to have an address book based on messages in your mailbox. An easy way to do this is to install addrlookup: get the source from <code>http://github.com/spaetz/vala-notmuch/raw/static-sources/src/addrlookup.c</code>, build with</li>
</ol>
<pre><code>cc -o addrlookup addrlookup.c `pkg-config --cflags --libs gobject-2.0` -lnotmuch</code></pre>
<p>and move the resulting binary into your path (all of this on your server), and then create a similar wrapper as for notmuch:</p>
<pre><code>~/bin/addrlookup:

#!/bin/bash
printf -v ARGS &quot;%q &quot; &quot;$@&quot;
exec ssh your_server addrlookup ${ARGS}</code></pre>
<p>And then add this to your <code>.emacs</code>:</p>
<pre><code>(require &#39;notmuch-address)
(setq notmuch-address-command &quot;/path/to/addrlookup&quot;)
(notmuch-address-message-insinuate)</code></pre>
<p>Now if you hit “TAB” after you start typing in an address, it will prompt you with completions (use up/down arrow to move between, hit enter to select).</p>
<h2 id="conclusion">Conclusion</h2>
<p>Congratulations! You now have a mail system that is more powerful than GMail and completely controlled by you. And there is a lot more you can do. For example, to enable encryption (to start, just signing emails), install <code>gnupg</code>, create a key and associate it with your email address, and add the following line to your .emacs and all messages will be signed by default (it adds a line in the message that when you send it causes emacs to sign the email. Note that this line must be the first line, so add your message <em>below</em> it):</p>
<pre><code>(add-hook &#39;message-setup-hook &#39;mml-secure-message-sign-pgpmime)</code></pre>
<p>An unfortunate current limitation is that the keys are checked by the notmuch commandline, so you need to install public keys on the server. This is fine, except that the emacs client installs them locally when you click on an unknown key (hit $ when viewing a message to see the signatures). So, at least for now, you have to manually add keys to the server with <code>gpg --recv-key KEYID</code> before they will show up as verified on the client (signing/encrypting still works, because that is done locally). Hopefully this will be fixed soon.</p>
<p><strong>Added July 9th, 2013:</strong></p>
<h2 id="addendum">Addendum</h2>
<p>Among the large amount of feedback I received on this post, many people recommended that I use Postfix and Dovecot over Exim and Courier. Postfix chosen because of security (Exim has a less than stellar history), and dovecot because it is simpler and faster than Courier (and more importantly, combined with Postfix frequently). Security is really important to me (as I want this system to be easy to mantain), so I decided to switch it. Since I’m not doing anything particularly complicated with the mail server / IMAP, the conversion was relatively straightforward. For people reading this, I’d suggest just doing this from the start (and substitute for the parts setting up Exim / Courier), but if you’ve already followed the instructions (as I have), here is what you should do to change. Note that I have gotten much of this information from guides at <a href="https://syslog.tv/">syslog.tv</a>, modified as needed.</p>
<ol style="list-style-type: decimal">
<li>Install postfix and dovecot with (accept the replacement policy):</li>
</ol>
<pre><code>sudo apt-get install dovecot-imapd postfix sasl2-bin libsasl2-2 libsasl2-modules procmail</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Add this to end of <code>/etc/postfix/main.cf</code>, to tell Postfix to use Maildir, sasl,</li>
</ol>
<pre><code>    home_mailbox = Maildir/

    smtpd_sasl_type = dovecot
    smtpd_sasl_path = private/auth
    smtpd_sasl_auth_enable = yes
    smtpd_sasl_security_options = noanonymous
    smtpd_sasl_local_domain = $myhostname
    broken_sasl_auth_clients = yes

    smtpd_sender_restrictions = permit_sasl_authenticated,
    permit_mynetworks,

    smtpd_recipient_restrictions = permit_mynetworks,
    permit_sasl_authenticated,
    reject_unauth_destination,
    reject_unknown_sender_domain,</code></pre>
<p>Add this to the end of /etc/postfix/master.cf:</p>
<pre><code>spamassassin unix - n n - - pipe
  user=spamd argv=/usr/bin/spamc -f -e
  /usr/sbin/sendmail -oi -f ${sender} ${recipient}</code></pre>
<p><strong>NOTE</strong>: It’s been pointed out to me that you may not have a <code>spamd</code> user on your system, this won’t work. So check that, and add the user if it’s missing.</p>
<p>And this at the beginning, right after the line <code>smpt inet n ...</code></p>
<pre><code>   -o content_filter=spamassassin</code></pre>
<p>And uncomment the line starting with ‘submission’ and put the following after it:</p>
<pre><code>  -o syslog_name=postfix/submission
  -o smtpd_tls_security_level=encrypt
  -o smtpd_sasl_auth_enable=yes
  -o smtpd_sasl_type=dovecot
  -o smtpd_sasl_path=private/auth
  -o smtpd_client_restrictions=permit_sasl_authenticated,reject
  -o smtpd_recipient_restrictions=reject_non_fqdn_recipient,
     reject_unknown_recipient_domain,permit_sasl_authenticated,reject</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><p>Change myhostname to be fully qualified if it isn’t. Confirm all is well with http://mxtoolbox.com</p></li>
<li><p>Set up dovecot. Edit /etc/dovecot/conf.d/10-mail.conf and change mail_location to:</p></li>
</ol>
<pre><code>maildir:~/Maildir/</code></pre>
<p>Edit /etc/dovecot/conf.d/10-master.conf and inside <code>service auth</code> comment the block that is there, and uncomment the one that is:</p>
<pre><code>  unix_listener /var/spool/postfix/private/auth {
    mode = 0666
  }</code></pre>
<p>Edit /etc/dovecot/conf.d/10-auth.conf and uncomment the line at the top:</p>
<pre><code>disable_plaintext_auth = yes</code></pre>
<p>Now to test that this is working, use <code>swaks</code> from a remote host, and run a command like:</p>
<pre><code>swaks -a -tls -q HELO -s mydomain.com -au myuser -ap &quot;mypassword&quot; -p 587</code></pre>
<p>And you should get a good response.</p>
<ol start="5" style="list-style-type: decimal">
<li>Filing mail with procmail.</li>
</ol>
<p>Delete <code>~/.forward</code> - we’ll be using procmail to put the mail in the Archive directory.</p>
<p>Put this in /etc/procmailrc</p>
<pre><code>DROPPRIVS=YES
ORGMAIL=$HOME/Maildir/
MAILDIR=$ORGMAIL
DEFAULT=$ORGMAIL</code></pre>
<p>Make ~/.procmailrc be:</p>
<pre><code>:0 c
.Archive/

:0
| /usr/local/bin/my-notmuch-new.sh</code></pre>
<p>This says to copy the message to the archive and then run my-notmuch-new.sh (which is a shell script that used to be called by incron). Technically it pipes the message to the script, but the script ignores standard in, so it is equivalent to just calling the script. Now fix the ownership:</p>
<pre><code>chmod 600 .procmailrc</code></pre>
<p>Remove incron, which we aren’t using anymore.</p>
<pre><code>sudo aptitude remove incron</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>Fix up spamassassin.</li>
</ol>
<p>Get the top of /etc/spamassassin/local.cf to look like:</p>
<pre><code>rewrite_header Subject
# just add good headers
add_header spam Flag _YESNOCAPS_
add_header all Status _YESNO_, score=_SCORE_ required=_REQD_ tests=_TESTS_ autolearn=_AUTOLEARN_ version=_VERSION_</code></pre>
<p>This adds the proper headers so that <code>afew</code> recognizes and tags as spam accordingly. And that should be it!</p>
<ol start="8" style="list-style-type: decimal">
<li>I’m not sure of a way to tell K9Mail that the certificate on the IMAP server has changed, so I just deleted the account and recreated it.</li>
</ol>
<p>Note: if you find any mistakes in this, or parts that needed additional steps, <a href="mailto:dbp@dbpmail.net">let me know</a> and I’ll correct/add to this.</p>
]]></description>
    <pubDate>Sat, 29 Jun 2013 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2013-06-29-hackers-replacement-for-gmail.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>A Literate Ur/Web Adventure</title>
    <link>http://dbp.io/essays/2013-05-21-literate-urweb-adventure.html</link>
    <description><![CDATA[<h2>A Literate Ur/Web Adventure</h2>

<p>by <em>Daniel Patterson</em> on <strong>May 21, 2013</strong></p>

<p><a href="http://www.impredicative.com/ur/">Ur/Web</a> is a language / framework for web programming that both makes it really hard to write code with bugs / vulnerabilities and also makes it really easy to write reactive, client-side code, all from a single, simple, codebase. But it is built on some pretty deep type theory, and while it is an incredibly practical research project, some corners of it still show - like error messages that scroll pages off the screen. I’ve experimented with it before, and have written a small application that is beyond a demo, but still small enough to be digestible.</p>
<p>For completeness and clarity, I present it here in complete literate style - all the files, interspersed with comments, are presented. They are split into sections by file, which are named in headings. All the text between the file name and the next file name that is not actual code is within comments (that is what the <code>#</code>, <code>(*</code> and <code>*)</code> are for), so you can copy the whole thing to the files and build the project. All the files should go into a single directory. It builds with the current version of Ur/Web. You can try out the application, as it currently exists (which might have been changed since writing this), at <a href="http://lab.dbpmail.net/dn">lab.dbpmail.net/dn</a>. The full source, with history, is available at <a href="http://hub.darcs.net/dbp/dnplayer">hub.darcs.net/dbp/dnplayer</a>.</p>
<p>The application is a video player for the daily news program <a href="http://democracynow.org">Democracy Now!</a>. The main point of it is to remember where in the show you are, so you can stop and resume it, across devices. It should work on desktop and mobile applications - I have targetted Chrome on Android, Chrome on computers, and Safari on iPhones/iPads. The main reason for not supporting Firefox is that it does not support the (proprietary) video/audio codecs that are the only format that Democracy Now! provides.</p>
<h2 id="dn.urp">dn.urp</h2>
<pre><code># .urp files are project files, which describe various meta-data about
# Ur/Web applications. They declare libraries (like random, which we&#39;ll
# see later), information about the database (both what it is named and
# where to generate the sql for the tables that the application is using).
# They separate meta-data declarations from the modules in the project by
# a single blank line, which is why we have comments on all blank lines
# prior to the end.
library random
database dbname=dn
sql dn.sql
# 
# They also allow you to rewrite urls. By default, urls are generated
# consistently as Module/function_name, which means that the main
# function inside Dn, our main module, is our root url. We can rewrite
# one url to another, but if we leave off the second, that rewrites to
# root. We can also strip prefixes from urls with a rewrite with a *.
# 
rewrite url Dn/main 
rewrite url Dn/*
# 
# safeGet allows us to declare that a function is safe to generate urls
# to, ie that it won&#39;t cause side effects. Along the same safety lines,
# we declare the external urls that we will generate and scripts we will
# include - making injecting resources hosted elsewhere hard (as Ur/Web
# won&#39;t allow you to create urls to anything not declared here).
#
# 
safeGet player
allow url http://dncdn.dvlabs.com/ipod/*
allow url http://traffic.libsyn.com/democracynow/*
allow url http://dbpmail.net/css/default.css
allow url http://dbpmail.net
allow url http://hub.darcs.net/dbp/dnplayer
allow url http://democracynow.org
allow url http://lab.dbpmail.net/dn/main.css
script http://lab.dbpmail.net/static/jquery-1.9.1.min.js
# One odd thing - Ur/Web doesn&#39;t have a static file server of its own, so
# you need to host any FFI javascript elsewhere. Here&#39;s where the javascript for
# this application, presented later, is hosted. For trying it out, leaving
# this the same is fine, though if you want to change the javascript, or
# not depend on my copy being up, you should change this and the reference in
# the application.
script http://lab.dbpmail.net/dn/dn.js
# 
# Next, we declare that we have foreign functions in a module called dnjs. This
# refers to a header file (.urs), and we furthermore declare what functions within
# it we are using. We declare them as effectful so that they aren&#39;t called multiple
# times (like Haskell, Ur/Web is purely functional, so normal, non-effectful functions are not
# guaranteed to be called exactly once - they could be optimized away if the compiler
# did not see you use the result of the function, and could be inlined (and thus
# duplicated) if it would be more efficient).
# 
ffi dnjs
jsFunc Dnjs.init=init
effectful Dnjs.init
jsFunc Dnjs.set_offset=set_offset
effectful Dnjs.set_offset

# The last thing we declare is the modules in our project. $/ is a prefix that means to
# look in the standard library, as we are using the option type (Some/None in OCaml/ML,
# Just/Nothing in Haskell, and very roughly a safe null in other languages). sourceL is
# a helper for reactive programming (to be discussed later). And finally, our main module,
# which should be last.
#         
$/option
sourceL
dn</code></pre>
<h2 id="dn.urs">dn.urs</h2>
<pre><code>(*</code></pre>
<p><code>.urs</code> files are header files (signature files), which declare all the public functions in the module (in this case, the <code>Dn</code> module). We only export our <code>main</code> function here, but all functions that have urls that we generate within the applications are also implicitly exported.</p>
<p>The type of main, <code>unit -&gt; transaction page</code>, means that it takes no input (<code>unit</code> is a value-less value, a placeholder for argumentless functions), and it produces a <code>page</code> (which is a collection of xml), within a <code>transaction</code>. <code>transaction</code>, like Haskell’s IO monad, is the way that Ur/Web handles IO in a safe way. If you aren’t familiar with IO in Haskell, you should go there and then come back.</p>
<pre><code>*)
val main : unit -&gt; transaction page</code></pre>
<h2 id="random.urp">random.urp</h2>
<pre><code># Random is a simple wrapper around librandom to provide us with random
# strings, that we use for tokens. We included it above with the line
# `library random`. Libraries are declared with separate package files,
# and here we link against librandom.a, include the random header, and declare
# that we are using functions declared in random.urs (that is the ffi line).
# We also declare that all three functions are effectful, because they have
# side effects
#
# NOTE: It has been pointed out that instead of doing this, we could either:
#       A. use Ur/Web&#39;s builtin `rand` function, and construct the strings
#          without using the FFI, or even easier:
#       B. just use the integers than `rand` generates as tokens.
#
#       I didn&#39;t realize that `rand` existed when I wrote this, but I&#39;m leaving
#       it in because it is a (concise) introduction to the FFI, which, given
#       the relatively small body of Ur/Web libraries, is probably something
#       you&#39;ll end up using if you build any large applications.
effectful Random.init
effectful Random.str
effectful Random.lower_str
ffi random
include random.h
link librandom.a</code></pre>
<h2 id="random.urs">random.urs</h2>
<pre><code>(*</code></pre>
<p>Like with main, we see that the signatures of these functions are ‘transaction unit’ and <code>int -&gt; transaction string</code>, which means the former takes no arguments, and the latter two take integers (lengths), and produce <code>string</code>s, within <code>transactions</code>. They are within <code>transaction</code> because they create side effects (ie, if you run them twice, you will likely not get the same result), and thus we want the compiler to treat them with care (as described earlier). Init seeds the random number generator, so it should be called before the other two are</p>
<pre><code>*)
val init: transaction unit
val str : int -&gt; transaction string
val lower_str : int -&gt; transaction string</code></pre>
<h2 id="random.h">random.h</h2>
<pre><code>/*</code></pre>
<p>Here we have the header file for the C library, which declares the same signatures as above, but using the structs that Ur/Web uses, and the naming convention that it expects (uw_Module_name).</p>
<pre><code>*/
#include &quot;types.h&quot;

uw_Basis_unit uw_Random_init(uw_context ctx);
uw_Basis_string uw_Random_str(uw_context ctx, uw_Basis_int len);
uw_Basis_string uw_Random_lower_str(uw_context ctx, uw_Basis_int len);</code></pre>
<h2 id="random.c">random.c</h2>
<pre><code>/*</code></pre>
<p>And finally the C code to generate random strings.</p>
<pre><code>*/
#include &quot;random.h&quot;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt; 
#include &quot;urweb.h&quot;

/* Note: This is not cryptographically secure (bad PRNG) - do not
   use in places where knowledge of the strings is a security issue.
*/

uw_Basis_unit uw_Random_init(uw_context ctx) {
  srand((unsigned int)time(0));
}

uw_Basis_string uw_Random_str(uw_context ctx, uw_Basis_int len) {
  uw_Basis_string s;
  int i;

  s = uw_malloc(ctx, len + 1);

  for (i = 0; i &lt; len; i++) {
    s[i] = rand() % 93 + 33; /* ASCII characters 33 to 126 */
  }
  s[i] = 0;

  return s;
}

uw_Basis_string uw_Random_lower_str(uw_context ctx, uw_Basis_int len) {
  uw_Basis_string s;
  int i;

  s = uw_malloc(ctx, len + 1);

  for (i = 0; i &lt; len; i++) {
    s[i] = rand() % 26 + 97; /* ASCII lowercase letters */
  }
  s[i] = 0;

  return s;
}</code></pre>
<h2 id="dn.ur">dn.ur</h2>
<pre><code>(*</code></pre>
<p>We’ll now jump into the main web application, having seen a little bit about how the various files are combined together. The first thing we have is the data that we will be using - one database table, for our users, and one cookie. The tables are declared with Ur/Web’s record syntax, where <code>Token</code>, <code>Date</code>, and <code>Offset</code> are the names of fields, and <code>string</code>, <code>string</code>, and <code>float</code> are the types.</p>
<p>All tables that are going to be used have to be declared, and Ur/Web will generate SQL to create them. This is, in my opinion, one weakness, as it means that Ur/Web doesn’t play well with others (as it needs the tables to be named uw_Module_name), and, even worse, if you rename modules, or refactor where the tables are stored, the names of the tables need to change - if you are just creating a toy, you can wipe out the database and re-initialize it, but obviously this isn’t an option for something that matters, and you just have to manually migrate the tables, based on the newly generated database schemas. Luckily the tables / columns are predictably named, but it still isn’t great.</p>
<pre><code>*)
(* Note: Date is the date string used in the urls, as the most
   convenient serialization, Offset is seconds into the show *)
table u : {Token : string, Date : string, Offset : float} PRIMARY KEY Token
cookie c : string
(*</code></pre>
<p>Ur/Web provides a mechanism to run certain code at times other than requests, called <code>task</code>s. There are a couple categories, the simplest one being an initialization task, that is run once when the application starts up. We use this to initialize our random library.</p>
<pre><code>*)
task initialize = fn () =&gt; Random.init
(*</code></pre>
<p>Part of being a research project is that the standard libraries are pretty minimal, and one thing that is absent is date handling. You can format dates, add and subtract, and that’s about it. Since a bit of this application has to do with tracking what show is the current one, and whether you’ve already started watching it, I wrote a few functions to answer the couple date / time questions that I needed. These are all pure functions, and all the types are inferred.</p>
<pre><code>*)
val date_format = &quot;%Y-%m%d&quot;

fun before_nine t =
    case read (timef &quot;%H&quot; t) of
        None =&gt; error &lt;xml&gt;Could not read Hour&lt;/xml&gt;
      | Some h =&gt; h &lt; 9
    
fun recent_show t =
   let val seconds_day = 24*60*60
       val nt = (if before_nine t then (addSeconds t (-seconds_day)) else t)
       val wd = timef &quot;%u&quot; nt in
   case wd of
       &quot;6&quot; =&gt; addSeconds nt (-seconds_day)
     | &quot;7&quot; =&gt; addSeconds nt (-(2*seconds_day))
     | _ =&gt; nt
   end
(*</code></pre>
<p>The server that I have this application hosted on is in a different timezone than the show is broadcasted in (EST), so we have to adjust the current time so that we can tell if it is late enough in the day to get the current days broadcast. Depending on what timezone your computer is, this may need to be changed.</p>
<pre><code>*)
fun est_now () =
    n &lt;- now;
    return (addSeconds n (-(4*60*60)))

(*</code></pre>
<p>We track users by tokens - these are short random strings generated with our random library. The mechanism for syncing devices is to visit the url (with the token) on every device, so the tokens will need to be typed in. For that reason, I didn’t want to make the tokens very long, which means that collisions are a real possibility. To deal with this, I set the length to be 6 characters, plus the number of tokens, log_26 (since users are encoded with lower case letters, n users can be encoded with log_26 characters, so we use this as a baseline, and add several so that the collision probability is low).</p>
<p>In this, we see how SQL queries work. You can embed SQL (a subset of SQL, defined in the manual), and this is translated into a query datatype, and there are many functions in the standard library to run those queries. We see here two: <code>oneRowE1</code>, which expects to get back just one row, and will extract 1 value from it. <code>E</code> means that it computes a single output expression. Note that it will error if there is no result, but since we are selecting the count, this should be fine. <code>hasRows</code> is an even simpler function; it simply runs the query and returns true iff there are rows.</p>
<p>Also note that we refer to the table by name as declared above, and we refer to columns as record members of the table. To embed regular Ur/Web values within SQL queries, we use <code>{[value]}</code>. These queries will not type check if you try to select columns that don’t exist, and of course does escaping etc.</p>
<pre><code>*)
(* linking to cmath would be better, but since I only
   need an approximation, this is fine *)
fun log26_approx n c : int =
    if c &lt; 26 then n else
    log26_approx (n+1) (c / 26)


(* Handlers for creating and persisting token *)
fun new_token () : transaction string =
    count &lt;- oneRowE1 (SELECT COUNT( * ) FROM u);
    token &lt;- Random.lower_str (6 + (log26_approx 0 count));
    used &lt;- hasRows (SELECT * FROM u WHERE u.Token = {[token]});
    if used then new_token () else return token

(*</code></pre>
<p>We write small functions to set and clear the tokens. We do this so that after a user has visited the unique player url at least once on each device, they will only have to remember the application url, not their unique url. <code>now</code> is a value of type <code>transaction time</code>, which gives the current time, and <code>setCookie</code>/<code>clearCookie</code> should be self explanatory.</p>
<pre><code>*)
fun set_token token =
    t &lt;- now;
    setCookie c {Value = token,
                 Expires = Some (addSeconds t (365*24*60*60)),
                 Secure = False}
    
fun clear_token () =
    clearCookie c

(*</code></pre>
<p>The next thing is a bunch of html fragments. Ur/Web doesn’t have a “templating” system, but it is perfectly possible to create one by defining functions that take the values to insert in. I’ve opted for a simpler option, and just defined common pieces. HTML is written in normal XML format, within <code>&lt;xml&gt;</code> tags, and like the SQL tags, these are typechecked - having attributes that shouldn’t exist, nesting tags that don’t belong, or not closing tags all cause the code not to compile.</p>
<p>There are a couple rough edges - some tags are not defined (but you can define new ones in FFI modules), and some attributes can’t be used because they are keywords (hence <code>typ</code> instead of <code>type</code>), but overall it is a neat system, and works very well.</p>
<pre><code>*)
fun heading () = 
    &lt;xml&gt;
        &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width&quot;/&gt;
        &lt;link rel=&quot;stylesheet&quot; typ=&quot;text/css&quot; href=&quot;http://dbpmail.net/css/default.css&quot;/&gt;
        &lt;link rel=&quot;stylesheet&quot; typ=&quot;text/css&quot; href=&quot;http://lab.dbpmail.net/dn/main.css&quot;/&gt;
    &lt;/xml&gt;

fun about () =
    &lt;xml&gt;
      &lt;p&gt;
      This is a player for the news program
      &lt;a href=&quot;http://democracynow.org&quot;&gt;Democracy Now!&lt;/a&gt;
      that remembers how much you have watched.
    &lt;/p&gt;
    &lt;/xml&gt;
    
fun footer () =
    &lt;xml&gt;
      &lt;p&gt;Created by &lt;a href=&quot;http://dbpmail.net&quot;&gt;Daniel Patterson&lt;/a&gt;.
        &lt;br/&gt;
        View the &lt;a href=&quot;http://hub.darcs.net/dbp/dnplayer&quot;&gt;Source&lt;/a&gt;.&lt;/p&gt;
    &lt;/xml&gt;

(*</code></pre>
<p>We now get to the web handlers. These are all url/form entry points, and do the bulk of the work. The first one, <code>main</code>, which we rewrote in <code>dn.urp</code> to be the root handler, is mostly HTML - the only catch being that if you have a cookie set, we just redirect you to the player.</p>
<p><code>getCookie</code> returns an <code>option CookieType</code> where <code>CookieType</code> is the type of the cookie (in our case, it is a string). <code>redirect</code> takes a <code>url</code>, and urls can be created from handlers (ie, values of type <code>transaction page</code>) with the <code>url</code> function. So we apply <code>player</code> which is a handler we’ll define later, to the token value (as a token is the parameter that <code>player</code> expects), and grab a url for that.</p>
<p>One catch to this is that Ur/Web doesn’t know that <code>player</code> isn’t going to cause side effects, which would mean that it shouldn’t have a url created for it (side effecting things should only be POSTed to), which was why we had to declare <code>player</code> as <code>safeGet</code> in <code>dn.urp</code></p>
<p>We also see a form that submits to <code>create_player</code>, which is another handler that we will define. One thing to note is that <code>create_player</code> is a <code>unit -&gt; transaction page</code> function - and the action for the submit is just <code>create_page</code>, not <code>create_page ()</code> - the action of submitting passes that parameter.</p>
<pre><code>*)
fun main () =
    mc &lt;- getCookie c;
    case mc of
        Some cv =&gt; redirect (url (player cv))
      | None =&gt; 
        return &lt;xml&gt;
          &lt;head&gt;
            {heading ()}
          &lt;/head&gt;
          &lt;body&gt;
            &lt;h2&gt;&lt;a href=&quot;http://democracynow.org&quot;&gt;Democracy Now!&lt;/a&gt; Player&lt;/h2&gt;
            {about ()}
            &lt;p&gt;
              You can listen to headlines on your way to work on your phone,
              pick up the first segment during lunch on your computer at work, and
              finish the show in the evening, without worrying what device you are
              on or whether you have time to watch the whole thing.
            &lt;/p&gt;
            &lt;h3&gt;How it works&lt;/h3&gt;
            &lt;ol&gt;
              &lt;li&gt;
                &lt;form&gt;
                  To start, if you&#39;ve not created a player on any device:
                  &lt;submit action={create_player} value=&quot;Create Player&quot;/&gt;
                &lt;/form&gt;
              &lt;/li&gt;
              &lt;li&gt;Otherwise, visit the url for the player you created (it should look like
                something &lt;code&gt;http://.../player/hcegaoe&lt;/code&gt;) on this device
                to synchronize your devices. You only need to do this once per device, after that
                just visit the home page and we&#39;ll load your player.
              &lt;/li&gt;
            &lt;/ol&gt;

            &lt;h3&gt;Compatibility&lt;/h3&gt;
            &lt;p&gt;This currently works with Chrome (on computers and Android) and iPhones/iPads.&lt;/p&gt;  
            {footer ()}
          &lt;/body&gt;
        &lt;/xml&gt;

(*</code></pre>
<p><code>create_player</code> is pretty straightforward, but it shows a different part of Ur/Web’s SQL support: dml supports INSERT, UPDATE, and DELETE, in the normal ways, with the same embedding as SQL queries (that <code>{[value]}</code> puts a normal Ur/Web value into SQL). We create a token, create a “user”, setting that they are on the current day’s show and at the beginning of it (offset 0.0), store the token, and then redirect to the player.</p>
<pre><code>*)
and create_player () =
    n &lt;- est_now ();
    token &lt;- new_token ();
    dml (INSERT INTO u (Token, Date, Offset)
         VALUES ({[token]}, {[timef date_format (recent_show n)]}, 0.0));
    set_token token;
    redirect (url (player token))

(*</code></pre>
<p>The next two functions encompass most of the player, which is the core of the application. The way that it is structured is a little odd, but with justification: Chrome on Android caches extremely aggressively, and doesn’t seem to pay attention to headers that say not to, which means that if you visited the application, and then a few days later open up Chrome again, it will seem like it is loading the page, but it is loading the cached HTML, it is not getting it from the server. This is really bad for us, because it means it will have both an old offset (in case you watched some of the show from another device), but worse, on subsequent days it will be trying to play the wrong day’s show! You can manually reload the page, but this is silly, so what we do is initially just load a blank page, and then immediately make a remote call to actually load the page. So what is cached is a little bit of HTML and some javascript that loads the page for real.</p>
<p>We do all of this is functional reactive style: we declare a <code>source</code>, which is a place where values will be put, and it will cause parts of the page (that are <code>signal</code>ed) to update their values. Then we set an onload handler for the body, which, first, makes an <code>rpc</code> call to a server side function (which is just another function, like all of these handlers), and then set the <code>source</code> that we defined to be the result of rendering the player. <code>render</code> is a client-side function that just creates the appropriate forms / html.</p>
<p>Finally, we will call a client-side function init, which will do some setup and then call into the javascipt ffi to the ffi <code>init</code> function, which will handle the HTML5 audio/video APIs (which Ur/Web doesn’t support, and are very browser specific anyway).</p>
<p>One incredibly special thing that is going on is the <code>SourceL.set os</code> that is passed to javascript. If you remember from our <code>.urp</code> file, we imported sourceL. It is a special reactive construct that allows you to set up handlers that cause side effects (are transactions) when the value inside the <code>SourceL</code> changes. So what is happening is we have created one of these on the server, in <code>player_remote</code>, and sent it back to the client. The client then curries the <code>set</code> function with that source, producing a single argument function that just takes the value to be updated. We hand this function to javascript, so that in our FFI code, we can just set values into this, and it can reactively cause stuff to happen in our server-side code.</p>
<p>The reactive component on the page is the <code>&lt;dyn&gt;</code> tag, which is a special construct that allows side-effect free operations on sources. <code>signal s</code> grabs the current value from the source <code>s</code>, and in this case we just return this, but we could do various things to it. The result of the block is what the value of the <code>&lt;dyn&gt;</code> tag is. In this case, we have just made a place where we can stick HTML, by calling <code>set s some_html</code>.</p>
<pre><code>*)
and player token =
    s &lt;- source &lt;xml/&gt;;
    return &lt;xml&gt;
      &lt;head&gt;
        {heading ()}
      &lt;/head&gt;
      &lt;body onload={v &lt;- rpc (player_remote token);
                    set s (render token v.Player v.Show);
                    init token v.Player v.Source (SourceL.set v.Source) v.Video v.Audio}&gt;
        &lt;dyn signal={v &lt;- signal s; return v}/&gt;
      &lt;/body&gt;
      &lt;/xml&gt;
(*</code></pre>
<p>The remote component is where most of the logic of the player resides. By now, you should be able to read most of what’s going in. Some points to highlight are the place where we create the <code>SourceL</code> that we will pass back, and set its initial value to offset. Also, <code>fresh</code> is a way of generating identifiers to use within html. Our render function will use this identifier for the player, which is necessary for the javascript FFI to know where it is. Finally, <code>bless</code> is a function that will turn strings into urls, by checking against the policy outlined in the <code>.urp</code> file for the application.</p>
<pre><code>*)
and player_remote token =
    n &lt;- est_now ();
    op &lt;- oneOrNoRows1 (SELECT * FROM u WHERE (u.Token = {[token]}));
    case op of
        None =&gt;
        clear_token ();
        redirect (url (main ()))
      | Some pi =&gt;
        set_token token;
        let val show = recent_show n
            val fmtted_date = (timef date_format show) in
            (if fmtted_date &lt;&gt; pi.Date then
                (* Need to switch to new day *)
                dml (UPDATE u SET Date = {[fmtted_date]}, Offset = 0.0 WHERE Token = {[token]})
            else
                return ());
            let val offset = (if fmtted_date = pi.Date then pi.Offset else 0.0)
                val video_url = bless (strcat &quot;http://dncdn.dvlabs.com/ipod/dn&quot;
                                              (strcat fmtted_date &quot;.mp4&quot;))
                val audio_url = bless (strcat &quot;http://traffic.libsyn.com/democracynow/dn&quot;
                                              (strcat fmtted_date &quot;-1.mp3&quot;)) in
            os &lt;- SourceL.create offset;
            player_id &lt;- fresh;
            
            return {Player = player_id, Show = show, Offset = offset,
                    Source = os, Video = video_url, Audio = audio_url}
            end
        end


(*</code></pre>
<p>The next three functions are simple - the first just renders the actual player. Note that we use the <code>player_id</code> we generated in <code>player_remote</code>. Then we provide a way to forget the player (if you want to unlink two devices, forget the player on one and create a new one), and due to some imperfections with how we keep the time in sync (mostly based on weirdness of different browsers implementations of the HTML5 video/audio APIs), to seek backwards, or start the show over, we need to tell the server explicitly, so we provide a handler to do that.</p>
<pre><code>*)
and render token player_id date =
     &lt;xml&gt;&lt;h2&gt;
       &lt;a href=&quot;http://democracynow.org&quot;&gt;Democracy Now!&lt;/a&gt; Player&lt;/h2&gt;
       {about ()}
       &lt;h3&gt;{[timef &quot;%A, %B %e, %Y&quot; date]}&lt;/h3&gt;
       &lt;div id={player_id}&gt;&lt;/div&gt;
       &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
       &lt;form&gt;
         &lt;submit action={start_over token} value=&quot;Start Show Over&quot;/&gt;
       &lt;/form&gt;
       &lt;form&gt;
         &lt;submit action={forget} value=&quot;Forget This Device&quot;/&gt;
       &lt;/form&gt;
       {footer ()}
     &lt;/xml&gt;

(* Drop the cookie, so that client will not auto-redirect to player *)
and forget () =
    clear_token ();
    redirect (url (main ()))

(* Because of browser quirks, this is the only way to get to an earlier time, synchronized *)
and start_over token () =
    dml (UPDATE u SET Offset = 0.0 WHERE Token = {[token]});
    redirect (url (player token))

(*</code></pre>
<p>Now we get to the last web handlers. The first one is a client side initializer. The main thing it sets up is a handler to <code>rpc</code> to the server whenever the offset <code>SourceL</code> changes. The call is to <code>update</code> (which we’ll define in a moment), and it optionally returns a new time to set the client to.</p>
<p>This may sound a little odd, but the basic situation is that you play part of the way through the show on one device, then pause, watch some on another device, and now hit play on the first device. It will POST a new time, but the server will tell it that it should actually be at a later time, and so we use the javascript FFI function <code>set_offset</code> to set the offset.</p>
<p>Finally we make it so that the client silently fails if the connection fails (this is bad behavior, but simple), and call the javascript FFI initialization function, which will set up the player and any HTML5 API related stuff.</p>
<pre><code>*)
and init token player_id os set_offset video_url audio_url =
    SourceL.onChange os (fn offset =&gt; newt &lt;- rpc (update token offset);
                                      case newt of
                                          None =&gt; return ()
                                        | Some time =&gt; Dnjs.set_offset time);
    offset &lt;- SourceL.get os;
    onConnectFail (return ());
    Dnjs.init player_id offset set_offset video_url audio_url

(*</code></pre>
<p>The last function is the simple handler that we called when the offset <code>SourceL</code> changes. It updates the time if the time is greater than the recorded offset (this is why we need the <code>start_over</code> handler), and otherwise returns the recorded offset to be updated.</p>
<pre><code>*)
and update token offset =
    op &lt;- oneOrNoRows1 (SELECT * FROM u WHERE (u.Token = {[token]}));
    case op of
         None =&gt; return None
       | Some r =&gt; (if offset &gt; r.Offset then 
                       dml (UPDATE u SET Offset = {[offset]}
                            WHERE Token = {[token]} AND {[offset]} &gt; Offset);
                       return None
                   else return (Some r.Offset))</code></pre>
<h2 id="sourcel.urs">sourceL.urs</h2>
<pre><code>(*</code></pre>
<p>This came from a supplemental standard library, and, as explained earlier, allows you to create <code>source</code>-like containers that call side-effecting handlers when their values change.</p>
<pre><code>*)
(* Reactive sources that accept change listeners *)

con t :: Type -&gt; Type

val create : a ::: Type -&gt; a -&gt; transaction (t a)

val onChange : a ::: Type -&gt; t a -&gt; (a -&gt; transaction {}) -&gt; transaction {}

val set : a ::: Type -&gt; t a -&gt; a -&gt; transaction {}
val get : a ::: Type -&gt; t a -&gt; transaction a
val value : a ::: Type -&gt; t a -&gt; signal a</code></pre>
<h2 id="sourcel.ur">sourceL.ur</h2>
<pre><code>(*</code></pre>
<p>The <code>sourceL</code>s are built on top of normal <code>source</code>s, and just call the <code>OnSet</code> function when you call <code>set</code>.</p>
<pre><code>*)

con t a = {Source : source a,
           OnSet : source (a -&gt; transaction {})}

fun create [a] (i : a) =
    s &lt;- source i;
    f &lt;- source (fn _ =&gt; return ());

    return {Source = s,
            OnSet = f}

fun onChange [a] (t : t a) f =
    old &lt;- get t.OnSet;
    set t.OnSet (fn x =&gt; (old x; f x))

fun set [a] (t : t a) (v : a) =
    Basis.set t.Source v;
    f &lt;- get t.OnSet;
    f v

fun get [a] (t : t a) = Basis.get t.Source

fun value [a] (t : t a) = signal t.Source</code></pre>
<h2 id="dnjs.urs">dnjs.urs</h2>
<pre><code>(*</code></pre>
<p>This is the signature file for our javascript FFI. It declares what functions will be exported to be accessible within Ur/Web, and what types they have.</p>
<pre><code>*)
val init : id -&gt; (* id for player container *)
           float -&gt; (* offset value *)
           (float -&gt; transaction unit) -&gt; (* set function *)
           url -&gt; (* video url *)
           url -&gt; (* audio url *)
           transaction unit

val set_offset : float -&gt; transaction unit</code></pre>
<h2 id="dn.js">dn.js</h2>
<pre><code>/*</code></pre>
<p>Since this is a adventure in Ur/Web, not Javascript, and there are plenty of places to learn about the quirks and features of HTML5 media APIs (and I don’t claim to be an expert), I’m just going to paste the code in without detailed commentary. The only points that are worth looking at are how we use <code>setter</code>, which you will remember is a curried function that will be updating a <code>SourceL</code>, causing rpcs to update the time. To call functions from the FFI, you use <code>execF</code>, and to force a transaction to actually occur, you have to apply the function (to anything), so we end up with double applications.</p>
<p>Other than that, all that is here is some browser detection (as different browsers have different media behavior) and preferences about media type in localstorage.</p>
<pre><code>*/
function init(player, offset, setter, video_url, audio_url) {
    // set up toggle functionality
    $(&quot;#&quot;+player).after(&quot;&lt;button id=&#39;toggle&#39;&gt;Switch to &quot; +
                        (prefersVideo() ? &quot;audio&quot; : &quot;video&quot;) + &quot;&lt;/button&gt;&quot;);
    $(&quot;#toggle&quot;).click(function () {
        window.localStorage[&quot;dn-prefers-video&quot;] = !prefersVideo();
        location.reload();
    });

    // put player on the page
    if (canPlayVideo() &amp;&amp; prefersVideo()) {
        $(&quot;#&quot;+player).html(&quot;&lt;video id=&#39;player&#39; width=&#39;320&#39; height=&#39;180&#39; controls src=&#39;&quot; +
                           video_url + &quot;&#39;&gt;&lt;/video&gt;&quot;);
    } else {
        $(&quot;#&quot;+player).html(&quot;&lt;audio id=&#39;player&#39; width=&#39;320&#39; controls src=&#39;&quot; +
                           audio_url + &quot;&#39;&gt;&lt;/audio&gt;&quot;);
    }

    // seek / start the player, if applicable
    if (isDesktopChrome()) {
        $(&quot;#player&quot;).one(&quot;canplay&quot;, function () {
            var player = this;
            if (offset != 0) {
                player.currentTime = offset;
            }
            player.play();
            window.setInterval(update_time(setter), 1000);
        });
    } else if (isiOS() || isAndroidChrome()) {
        // iOS doesn&#39;t let you seek till much later... and won&#39;t let you start automatically,
        // so calling play() is pointless
	$(&quot;#player&quot;).one(&quot;canplaythrough&quot;,function () {
	    $(&quot;#player&quot;).one(&quot;progress&quot;, function () {
		if (offset != 0) {
                    $(&quot;#player&quot;)[0].currentTime = offset;
                }
                window.setInterval(update_time(setter), 1000);
	    });
	});   
    } else {
        $(&quot;#player&quot;).after(&quot;&lt;h3&gt;As of now, the player does not support your browser.&lt;/h3&gt;&quot;);
    }
}

function set_offset(time) {
    var player = $(&quot;#player&quot;)[0];
    if (time &gt; player.currentTime) {
        player.currentTime = time;
    }
    
}

// the function that grabs the time and updates it, if needed
function update_time(setter) {
    return function () {
        var player = $(&quot;#player&quot;)[0];
        if (!player.paused) {
            // a transaction is a function from unit to value, hence the extra call
            execF(execF(setter, player.currentTime), null)
        }
    };
}

// browser detection / preference storage

function canPlayVideo() {
    var v = document.createElement(&#39;video&#39;);
    return (v.canPlayType &amp;&amp; v.canPlayType(&#39;video/mp4&#39;).replace(/no/, &#39;&#39;));
}

function prefersVideo() {
    return (!window.localStorage[&quot;dn-prefers-video&quot;] || window.localStorage[&quot;dn-prefers-video&quot;] == &quot;true&quot;);
}

function isiOS() {
    var ua = navigator.userAgent.toLowerCase();
    return (ua.match(/(ipad|iphone|ipod)/) !== null);
}

function isDesktopChrome () {
    var ua = navigator.userAgent.toLowerCase();
    return (ua.match(/chrome/) !== null) &amp;&amp; (ua.match(/mobile/) == null);
}

function isAndroidChrome () {
    var ua = navigator.userAgent.toLowerCase();
    return (ua.match(/chrome/) !== null) &amp;&amp; (ua.match(/android/) !== null);
}</code></pre>
<h2 id="makefile">Makefile</h2>
<p>To actually build our application, we have to first build our C library. Then we’ll build the app, using the sqlite backend. To get this running, we then need to do <code>sqlite3 dn.db &lt; dn.sql</code> (note you only need to do this once) and then start the server with <code>./dn.exe</code>. You can then visit the application at <code>http://localhost:8080</code>. This has been tested on current Debian Linux and Mac OSX.</p>
<pre><code>all: app

librandom.a: librandom.o
	ar rcs $@ $&lt;

librandom.o: random.c
	gcc -I/usr/local/include/urweb -g -c -o $@ $&lt;

app: dn.ur dn.urs dn.urp librandom.a
	urweb -dbms sqlite -db dn.db dn

.PHONY: clean

clean:
	rm -f librandom.a librandom.o</code></pre>
]]></description>
    <pubDate>Tue, 21 May 2013 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2013-05-21-literate-urweb-adventure.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>Programming as Literature</title>
    <link>http://dbp.io/essays/2012-10-24-programming-literature.html</link>
    <description><![CDATA[<h2>Programming as Literature</h2>

<p>by <em>Daniel Patterson</em> on <strong>October 24, 2012</strong></p>

<p>Sometimes I’m not sure how to explain what I study or why I study it. I tell people that I study theoretical computer science, or algorithms and programming languages, or math and computer science, and if they ask why? Let’s come back to that. First I want to talk about literacy.</p>
<p>Literacy is about being able to understand the recorded thoughts of other people, and being able to share your own in a permanent medium. There are beautiful oral traditions, but most stories, much of human knowledge, is written down. Literacy allows one to tap into that sea of knowledge. In many ways, libraries are one of humanity’s greatest achievements; that one can walk into a building that contains the thoughts and discoveries of thousands of people, stretching back hundreds or thousands of years (and as long as you aren’t at an exclusive university, you can often access that information for free). Some knowledge is certainly more accessible than other knowledge, and languages of course complicate things, but the essential element of literacy is both the perception of the world around you and the ability to describe it and share that with others. We must be able to understand the thoughts of others and formulate our own so that others can understand them.</p>
<p>The broader and perhaps more important aspect of literacy is that it allows you to contextualize your own life and perceptions in relation to others. In writing, you turn your own lived experience into something you can share. In reading, you realize that others have lived experiences that are in some ways similar and in others different from your own. In many ways, literacy is broader than reading and writing, it is rather about developing perspective on your own life and understanding of the lives of others. I can remember as a small child looking up at an airplane and realizing for the first time that there were people inside of it, in the middle of their own lives, with their own thoughts, hopes, dreams. For the first time I had an empathetic sense that I was not the center of the world (Descartes be damned).</p>
<p>Now, you may be asking, with good reason, what does this have to do with computer science? I want to argue that one of the primary mediums of our lives is now something that most of us do not have literacy in. We communicate with one another with email, websites, cell phones, etc. We learn information by pushing a button on a piece of electronics that displays pictures to us that change as we touch them or use devices attached to it. Traffic lights and airline schedules are planned with computers, cars run with them, watches, microwave ovens. Most things we plug in or have batteries have computers in them. Much of our lives are carried out using computers that we don’t have more than a surface empirical understanding of. Now there have always been things that individuals don’t understand. Tax codes, foreign languages, specifics of geography, etc.</p>
<p>But there are a couple interesting things about computers that distinguish them. The first is that they are all essentially the same. There is an underlying similarity between all computers, and indeed even among all possible devices that can compute. This means that it actually is possible to learn about all of these things.</p>
<p>The second is that they are primarily designed as a way for humans to express their thoughts. We don’t think about computers in this sense very much, but it is what distinguishes them from most other machines - they are used so that one person can express how to do something and share it with others. They are a medium for talking about solving problems. The breadth of such problems that they can express is visible by looking at all the places that they are used now - and imagine, this is with only a small minority of the population thinking up ways to use them!</p>
<p>There is a third dimension that is similarly interesting, and talked about more, which is that they are a way to expand our own mental capacities - if I am confronted with a task of sorting a few hundred (or thousand) documents, I can do it by hand, or, if I know how, I can write a program to do it and get a computer to carry out the work of sorting (and if I wanted the computer to do this sorting every day for the next year, I wouldn’t have to do any more work). What this means is that not only are they a way for me to share my ideas of how to solve a problem, they are also a way to automate that very problem solving.</p>
<p>What is interesting and sad is that while the posession of computers is expanding rapidly, the knowledge of how to truly use them is not. People are sold devices that allow them to perform a set number of functions (all of which are simply repetitions of thoughts by the people working at the company who sold them the device), but they are not given the tools to express their own thoughts, to expand their own mental capacity in any way other than that already thought of by someone else. We have expanded the medium without expanding literacy. And indeed, there is a financial explanation for this. It’s hard to sell knowledge when people can create it themselves. Many technological “innovations” these days are trivial combinations of earlier ideas which would be unnecessary if people were able to carry out those kinds of compositions themselves.</p>
<p>So why am I interested in computer science? I’m interested in it because I am interested in human thought. I am interested in how people solve problems, and seeing problems that others have solved. I am interesting in teaching people how to express themselves in this medium, and learning it myself. I study programming as literature, to read, to write, to share. I study it to figure out the world we live in, and imagine how else it could be.</p>
]]></description>
    <pubDate>Wed, 24 Oct 2012 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2012-10-24-programming-literature.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>Haskell / Snap ecosystem is as productive as Ruby/Rails.</title>
    <link>http://dbp.io/essays/2012-04-26-haskell-snap-productive.html</link>
    <description><![CDATA[<h2>Haskell / Snap ecosystem is as productive as Ruby/Rails.</h2>

<p>by <em>Daniel Patterson</em> on <strong>April 26, 2012</strong></p>

<p>This may be controversial, and all of the usual disclaimers apply - this is based on my own experience using both of the languages/frameworks to do real work on real projects. Your mileage may vary. Because this is something that has the potential to spiral into vague comparisons, I am going to try to compare points directly, based on things that I’ve experienced. I am not going to say “I like Haskell better” or anything like that, because the point of this is not so much to convince people about the various merits of the languages involved, just to point out that I’ve found that they both are as productive (or that Snap feels more so). For Haskell programmers, this could be an indication to try out the web tools that you have available, especially if you are usually a Rails developer.</p>
<p>As a note - some of this could also apply to other haskell web frameworks (in particular, most of this pertains to happstack, and some pertains to yesod), but since Snap is what I use, I want to keep it based on my own personal experience.</p>
<p><code>1.</code> The number one productivity improvement is a smart strong type system. This is less of an issue for small projects, but as soon as you have at least a few thousand lines of code, adding new features or refactoring inevitably involves changes to multiple parts of the codebase. Having a compiler that will tell you all the places that you need to change things is an amazing productivity booster. This can be approximated in some ways with good test coverage, but it is really a different beast - tests often need to be changed as well, and if you aren’t very careful about this it is easy to change them in ways that don’t catch new bugs. Additionally, it is hard (or very tedious, if you do it wrong) to achieve high enough coverage to actually catch all of the bugs introduced in refactoring. This as compared to a compiler that is completely automated and will always be aware of all of the code you have and the ways that it interacts (at least to the extent that you actually use the type system - but if you are a good haskell programmer, you will).</p>
<p>This alone wouldn’t be enough to suggest using Haskell/Snap over Ruby/Rails, as a type system isn’t worth much without supporting libraries, but as I switch between the ecosystems, this is the place where I notice the most drastic improvements in productivity, so I put it first.</p>
<p><code>2.</code> Form libraries. There are many different libraries for dealing with forms in Rails, and there is the built in one as well. The general idea is that you define some validations on your models, and then use the DSLs from the form libraries to define forms, and can do validations, etc. In Haskell (in my opinion), the best form library is Digestive-Functors (thanks Jasper!), and the productivity difference is staggering in more complex use-cases. In the sort of vanilla examples that rails has, the validation system works quite well, and dynamic introspection allows you to write really short forms. This begins to break down when you start getting forms that don’t correspond in a simple way to models. I have forms that are sometimes a mix of two models, or forms that are a partial view into a data structure, or any number of other variations.</p>
<p>With Digestive-Functors, I can define the forms that I need, and re-use components between multiple forms (forms are composable), and these validations are on the form, not on the underlying model. It is obviously useful to database level data integrity checks, but I find that having them being the main / only way of doing validations is really limiting - because sometimes there are special cases when you want the validation done one way and other times another.</p>
<p>More generally, it is possible that the business logic of a specific form may have requirements that do not always have to hold for the datastore, and thus should not reside in the integrity checks. Having written a lot of forms (who hasn’t?), I find that getting the first form out is much faster with Rails, but inevitably when I need to change something it starts become difficult fast. Every time I am doing it I keep picturing an exponential curve - sure it starts out really small, but it gets really big really fast! It isn’t that I run into things that are not possible with Rails, but they end up being more difficult, more error prone, and generally reduce my productivity. With Digestive-Functors, I spend a little more time building the forms in the beginning, but I’ve never had requirements for a form that weren’t easily implemented (almost without thinking).</p>
<p><code>3.</code> Routing is the next big one. This may be more of an opinion that the previous ones, but I have always thought that great care should be involved in designing the url structure of a site. In this sense, I guess I disagree with the idea of universally using REST - I think it is very useful when writing APIs, but when designing applications for people, I believe the urls should be meaningful to the people, not to machines. Usually, right after modeling the data of an application, I make a site-map - this is a high level view of what the site should look like. Instead, with Rails, I spend time thinking of how I can adapt what I want to the REST paradigm, and usually end up with something that is an incomplete/counterintuitive representation.</p>
<p>More broadly, I think the idea of hierarchical routing is brilliant - the idea that you match routes by pieces. What this allows you to do is easily abstract out work that should be done for many different related requests. In Rails, this is approximated by :before_filters (ie, it a controller for a specific model, you might fetch the item from the id for many different handlers), but it is a poor substitute. For example I often have an “/admin” hierarchy, and to limit this, all I have to do is have one place (the adminRouter or something) that does the required work to ensure only administrators can access, and it can also fetch any data that is needed, and then it can pass back into the route parsing mode. Or if I want to do the rails-style pre-fetching, then I design the routes as “/item/id/action” and have a handler that matches “/item/id”, fetches the item, and then matches against the various actions. If I have nested pieces of data, this is just as easy. I could have “item/id/something/add” which adds a new “something” to the item with id “id”, This would all be in the same hierarchy, so the code to fetch the item would still only exist once.</p>
<p>Not only is this very natural to program, it keeps the flow easy to follow when you are looking back at it, and allows backtracking in a great way: if, in a handler, you reach something that indicates that this cannot be matched, like if the path was “/item/id” but the id did not correspond to an actual item, you can simply “pass” and the route parser continues looking for things that will handle the request. If it finds nothing, it gives a 404.</p>
<p>An example of how you could exploit things in a really clean way - if you are building a wiki-like site, then you first have a route that matches “/page/name” and looks up the page with name “name”. If it doesn’t find it, it passes, and the next handler can be the “new page” handler, that prompts the user to create the page. As with everything else, I’m not saying this cannot be done with Rails, simply that it is much more natural and easy to understand with Snap (and Happstack, where this routing system originated, at least in the Haskell world).</p>
<p><code>4.</code> Quality of external libraries. Point 2 was a special case of this, since dealing with forms comes up so much, but I think the general quality of libraries in Haskell is superb. One example that I came up against was wanting to parse some semi-free-form CSV data into dates and times. Haskell has the very mature parsing library Parsec (which has ports into many languages, including Ruby) that makes it really easy to write parsers. I ported an ad-hoc parser to it, and found that not only was I able to write the code in a fraction of the time, but it was a lot more robust and easy to understand.</p>
<p>For testing of algorithmic code, the QuickCheck library is pretty amazing - in it, you tell it how to construct domain data, and then certain invariants that should hold over function applications, and it will fuzz-test with random/pathological data. The first time you write some of these tests (and catch bugs!) you will wonder why you haven’t been testing like that before! I don’t really want to go into it here, but the other point is that many of these libraries are very very fast - there has been, over the last couple years, a massive push to have very performant libraries, with a lot of success. The Haskell web frameworks webservers regularly trounce most other webservers, and there are very high performant json, text processing, and parsing libraries (attoparsec is a version of parsec that is very fast).</p>
<p><code>5.</code> Templating. In this, I want to directly compare the experience of using Heist (a templating system made by the Snap team) and Erb/Haml (I mostly use the latter, but in some things, like with javascript, I have to use the former). The first big difference is the idea of layouts/templates/partials in rails. I never really understood why there was this distinction when I first used it, and when comparing it to Heist (which has no distinction - any template can be applied to another, to achieve a layout like functionality, and any template can be included within another, to achieve a partial like functionality) it feels very limited.</p>
<p>The other major difference is that the two templating languages in Ruby allow dynamic elements by embedding raw ruby code, whereas the former allows dynamic stuff by allowing you to define new xml tags (called splices) that you can then use in the templates. I have found this to be an extremely powerful idea, as it allows you to not only do all the regular stuff (insert values, iterate over lists of values and spit out html), but can even allow you to build custom vocabularies of elements that you want to use that are designed to go with javascript (so for example, I built an asynchronous framework on top of this, where I had a “<code>&lt;form-async&gt;</code>” tag and “<code>&lt;div-async&gt;</code>”s that would be replaced asynchronously by the responses from the form posts).</p>
<p>It also adapts to being used with (trusted) user generated input - I’ve used it in multiple CMS systems so that, for example, all links to external sites are set to open in new tabs/windows (by overriding the “<code>&lt;a&gt;</code>” tag and adding the appropriate “target”) or allowing the users to gain certain dynamic stuff for their pages. Compared to this, the situation with Haml always seems hopelessly tied up with ruby spaghetti code - not that it always is (you can always be careful), but the split with Heist both feels like a cleaner separation AND more powerful, which is not something you get often, and I think is a sign that the metaphor that Heist created (which is based on a couple really simple primitives) is really something special.</p>
<p><code>6.</code> This is sort of an extension of the first point, and I’m putting it towards the end because it is the most subjective of this already quite subjective comparison - I think that web applications built with Haskell/Snap are much easier to edit / add to than corresponding applications in Ruby/Rails. One of the biggest reasons for this is that there is much more boilerplate/code spread in ruby - some of it is auto-generated, other bits is manually generated, but there ends up being code scattered around. It is pretty easy to add new code, but when you want to edit / refactor existing code, it starts to get hard to figure out where everything is. A bit of this relies on conventions to a degree (which you learn), but there is simply less code in Snap, and usually everything pertaining to a specific function is in one place. This has a lot to do with the functional paradigm - there is no hidden state, so generally all the transformations that occur are very transparent, whereas with Rails it is possible for stuff from the ApplicationController being applied, or just various filters coming into play, or stuff from the model, etc. There is no obvious “starting point” if you want to see how a request travels through your application (candidates include the routes file, the controllers, etc), in the same way where with a Snap application, the code to start the web server is in one of the files you write! You can trace exactly what it is doing from there!</p>
<p>In addition, there is also very little “convention” with Snap. It enforces nothing, which has the consequence (in addition to allowing you to make a mess!) of having the whole application conforming to exactly how you think it should be organized. I’ve found that this actually makes it much easier to add new things or modify existing functionality (fix bugs!), because the entire structure of the application, from how the requests are routed to how responses are generated, is based on code I wrote. This means that making a change anywhere in this process is usually very easy - it feels in some ways like the difference in making a change to an application you wrote from scratch and one that you picked up from someone else. There is also a potential downside to this - the first couple applications I built had drastically different organizational systems</p>
<p>(Side note for anyone reading this who is curious: I’ve converged to the following method: all types for the application lives in a Types module or hierarchy, all code that pertains to the datastore lives in a State hierarchy or module in a small application, code for splices lives in a Splices hierarchy, forms live is a Forms hierarchy, and the web handlers live in a Handlers hierarchy. I also usually have a Utils module that collects some various things that are used in all sort of different places. Everything depends on Types and Utils. Splices, Forms, and State are all independent of one another, and Handlers depends on everything. And then of course there is an Application module and Main, according to the generated code from Snap).</p>
<p>This is a major difference in how Snap even differs from some other Haskell web frameworks, that it seems more like a library with which to build a web application instead of a true framework, but in my experience this is actually a really powerful thing, and makes the whole process a lot more enjoyable, because I never feel like I’m trying to conform to how someone else thinks I should organize things.</p>
<p><code>7.</code> I’m bundling the performance, security, etc all at once. Rails is a very stable framework, so lots of work has gone into this. But I think the recent vulnerabilities exposed on a lot of major sites (like GitHub) based on the common paradigm of mass-assignment sort of point out the negative side. Snap is much newer, but it was built with security in mind from the beginning, as far as I can tell, and most libraries that I have used have also mentioned ways that it comes up - the entire development community seems a lot more aware / concerned with it.</p>
<p>I think part of this probably has to do with the host languages - ruby is a very dynamic language that has a history of experimentation (so generally, flexibility is preferred of correctness), whereas Haskell is a language where lots of static guarantees are valued, and security is usually lumped in with correctness. For performance, there is no question that Haskell will win hands down on any performance comparison (and on multithreading). Granted, a lot of web code is disk/database bound so this isn’t a huge deal, but it is nice to know that you aren’t needlessly wasting cycles (and can afford to run on smaller servers).</p>
<p><code>8.</code> Now, as a counterpoint, I want to articulate what Rails really has over Snap. Number one, and this is huge, is the size of the community. There are a massive number of developers who know how to use Rails (how many are good at it is another question), and this also means that if you are trying to do something it is much more likely that a prebuilt solution exists. It also means that it will be easier to hire people to work on it, and easier to sell it as a platform to clients/bosses.</p>
<p>The Haskell community is surprisingly productive given its size (and some of the tools it has produced are amazing - examples mentioned in this comparison are Parsec, QuickCheck, Digestive-Functors, etc), but there is some sense where they will always be at a disadvantage. This means that if you are doing any sort of common task with Rails, there will probably be a Gem that does it. The unfortunate part is that sometimes the Gem will be unmaintained, partially broken, incompatible, as the quality varies widely. This is a place where a lot of subjectivity comes in - I have found that most of what I need exists in the haskell ecosystem, and if stuff doesn’t it isn’t hard to write libraries, but this could be a big dealbreaker for some people.</p>
<p>Cheers, and happy web programming.</p>
]]></description>
    <pubDate>Thu, 26 Apr 2012 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2012-04-26-haskell-snap-productive.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>Math/Science integrated with Scheme</title>
    <link>http://dbp.io/essays/2011-12-06-science-scheme.html</link>
    <description><![CDATA[<h2>Math/Science integrated with Scheme</h2>

<p>by <em>Daniel Patterson</em> on <strong>December  6, 2011</strong></p>

<p>I had an idea today, of an interactive homework assignment for a Chemistry class. It was a prompt, and you could type in queries and it would give responses. The basics would be:</p>
<pre><code># (questions)
=&gt; To Do: (1,2,3,4,5,6,7)
   Complete: ()
# (question-1)
=&gt; 1. How many grams of Na are needed to make 28 grams of NaCl?
# (periodic-table &#39;Na)
=&gt; Sodium - Atomic Number 11 - Weight 22.98976928
# (periodic-table &#39;Cl)
=&gt; Chlorine - Atomic Number 17 - Weight 35.453
# (* 22.98976928 (/ 28 (+ 22.98976928 35.453)))
=&gt; 11.01
# (answer-1 11.01)
=&gt; Correct! Great job. 1/7 Questions completed.
# (questions)
=&gt; To Do: (2,3,4,5,6,7)
   Complete: (1)</code></pre>
<p>Now if you don’t know Scheme syntax, the line with the numeric calculation might be a little confusing, but once you realize that it is just pure prefix notation (the operator always comes first, every set of parenthesis wrap an operation) it should start making sense. I’m pretty sure I could explain Scheme to anyone who is taking high-school science in an hour, but the three sentence explanation is: Every expression is wrapped inside parenthesis. The first word inside the parenthesis is a function, the rest (there don’t have to be any) are arguments to the function, which can be be other expressions or basic items like numbers or strings. Arithmetic follows this pattern, which may seem a little unnatural at first, but this consistency means that you now know almost all there is to know about Scheme.</p>
<p>But what I’ve described here isn’t actually much better than the web question and answer system that I saw today, that gave me this idea. It’s basically just an interactive text-based version of the same thing. What I started thinking of is having the capability to add things like this:</p>
<pre><code># (assignment-equations)
Arrhenius equation - k = Ae^(-Ea/RT)
  provided versions:
  (arrhenius-k frequency-factor activation-energy temperature)
  (arrhenius-a reaction-coefficient activation-energy temperature)
...
# (arrhenius-k 1.01e11 13500 273)
2.6375e8</code></pre>
<p>Which would provide both references and ways to do some of the more boring rote work quickly. Descriptions of the equations could also exist, making it even more of an interactive learning project. But what would be even better would be to allow students to define new functions (or redefine old ones) on the fly. Let’s say there are a bunch of different calculations that require the same involved steps. I saw today I student working through two laborious calculations, which differed only in that the value for the activation energy. What would be amazing is if a student could do something like:</p>
<pre><code># (define (my-arrh-eq act-energy)
    (arrhenius-k (arrhenius-a 2.75e-2 act-energy 293) act-energy 333))
=&gt; Defined new function my-arrh-eq!
# (my-arrh-eq 14500)
=&gt; 1.01</code></pre>
<p>I don’t remember if that was the answer or even the value for the activation energy (it probably isn’t), but that was the general solution. Now the problem was that a rate coefficient (2.75e-2) was given for 20degrees celsius and and the problem asked what was the rate coefficient for 60degrees celsius (same reaction). The problem was posed with two different activation energies, and identical and reasonably involved calculations resulted - using the given 20degree setup to solve for the frequency factor and then plug that into the same equation this time using 60degrees.</p>
<p>But what was interesting about this problem was the technique of solving one equation and using a part of that in the other - not actually doing out the arithmetic. It would be amazing if a student could build things like the function above, which clearly demonstrate an understanding of the technique, but also reveal a capacity to organize their thoughts and string together the pieces into higher level abstractions - a critical part of the type of thinking that underlies computer programming, and something that is going to become more and more important as time goes on.</p>
<p>I think there is amazing potential to systems like this - where programming is built into the fabric of math and science work, because it will both teach students to program (which is a very helpful thing), but it will also focus their attention and mental efforts on understanding how to string together concepts and actually solve problems, not just how to do calculations. I think it could also have a motivating effect because when you start writing programs like this, you feel like you are somehow getting out of doing boring work (which you are), and that you must be cheating somehow (and that feels good!). Little do you know that you are actually learning the material better than the person who did the calculations out by hand, because you focused on what was really important and had to figure out the general solution.</p>
<p>Now some of this is already happening - probably mostly using TI-BASIC on graphing calculators, but the system is reasonably unnatural (and no one is teaching students how to use it) and removed from basic work that I don’t think it is very widespread. I think a system that students would interact with that would allow them to build functions and use existing ones in the course of doing work would be a really amazing thing, both for their understanding of the subject itself and also to learn computer programming (or, more generally, “algorithmic thinking”).</p>
]]></description>
    <pubDate>Tue, 06 Dec 2011 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2011-12-06-science-scheme.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>
<item>
    <title>iOS is anti-UNIX and anti-programmer.</title>
    <link>http://dbp.io/essays/2011-09-15-ios-anti-unix.html</link>
    <description><![CDATA[<h2>iOS is anti-UNIX and anti-programmer.</h2>

<p>by <em>Daniel Patterson</em> on <strong>September 15, 2011</strong></p>

<p>When I was first learning about UNIX, and learning to use Linux, the most immediately powerful tool that I found was the shell’s pipe operator, ‘|’. Using the commandline (because at that point, linux GUI’s were not so well developed, and the few distros that tried to allow strictly graphical operation usually failed miserably) was at times difficult, and at times rewarding, but it was the pipe that opened up a whole world for me.</p>
<p>I can remember looking through an online student directory in highschool that had names, email addresses, etc. For student government elections it had become popular (if incredibly time consuming) to copy and paste the hundreds of email addresses and send a message to the every student. For me, with my newfound skills, it amounted to something like:</p>
<pre><code>cat directory.txt | grep @ | awk &#39;{print $3}&#39; | perl -pe &#39;s/\n/,/&#39;</code></pre>
<p>It seemed like magic at the time, and in some ways, it still does. What the shell (and UNIX in general) offered was composability - it gave you simple (but powerful) tools, and a standard way of linking them together - text streams. By combining those together, it offered immeasurable power, much more than any single tool. The mathematics of combinations guarantees this.</p>
<p>The more I use graphical interfaces (or anything that does not operate on text streams - commandline curses programs included), the more I am struck by how profound the loss of composability is - each program has to try to implement all the standard things (searching, sorting, transforming) that you might want to do with the information it has, and in that repetition lies inconsistencies and usually plain lack of power. The better ones share common libraries, and gain common functionality, but this only amounts to their least common denominator - two separate programs can not (easily) expose their higher functionality to each other (at least not it compiled languages) in the way that commandline stream processing programs can.</p>
<p>What I realized the other day, is that iOS is the extreme example of that lack of flexibility, taken almost to the point of caricature - the only interaction that is possible is through single applications that for the most part can have no connection to other applications. People rejoiced when copy and paste was added, but that celebration hides a sad loss of the true power that computers have. The existence of files - the only real way that composability is achieved in GUI systems (ie, do one thing, save the file, open with another program, etc) - has been essentially eliminated, and applications must therefore do everything that a user might want to do with whatever data they have or will get from the user.</p>
<p>I’d noticed before how frustrating it was for me to use iOS, but I wasn’t sure until recently exactly why that was, until I realized that it had effectively taken away the one thing that is so fundamental about computers, and why I am a programmer - the ability to compose. Every day I live and breath abstraction, and building things out of different levels of it, and the idea of not being able to combine various parts to make new things is so antithetical to that type of thinking that I almost can’t imagine that iOS was created by programmers. I remember looking at the technical specifications of the most recent iPhone and thinking - that is a full computer, and it’s small enough to fit in a pocket - that is a profound change in the way the world works. But it’s not a computer, it’s just a glorified palm pilot with a few bells and whistles.</p>
]]></description>
    <pubDate>Thu, 15 Sep 2011 00:00:00 UT</pubDate>
    <guid>http://dbp.io/essays/2011-09-15-ios-anti-unix.html</guid>
    <dc:creator>Daniel Patterson</dc:creator>
</item>

    </channel>
</rss>
